/*
 * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
 * in compliance with the License. You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License
 * is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
 * or implied. See the License for the specific language governing permissions and limitations under
 * the License.
 */
/*
 * This code was generated by https://github.com/googleapis/google-api-java-client-services/
 * Modify at your own risk.
 */

package com.google.api.services.dataproc.model;

/**
 * The cluster config.
 *
 * <p> This is the Java data model class that specifies how to parse/serialize into the JSON that is
 * transmitted over HTTP when working with the Cloud Dataproc API. For a detailed explanation see:
 * <a href="https://developers.google.com/api-client-library/java/google-http-java-client/json">https://developers.google.com/api-client-library/java/google-http-java-client/json</a>
 * </p>
 *
 * @author Google, Inc.
 */
@SuppressWarnings("javadoc")
public final class ClusterConfig extends com.google.api.client.json.GenericJson {

  /**
   * Optional. Autoscaling config for the policy associated with the cluster. Cluster does not
   * autoscale if this field is unset.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private AutoscalingConfig autoscalingConfig;

  /**
   * Optional. The node group settings.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.util.List<AuxiliaryNodeGroup> auxiliaryNodeGroups;

  static {
    // hack to force ProGuard to consider AuxiliaryNodeGroup used, since otherwise it would be stripped out
    // see https://github.com/google/google-api-java-client/issues/543
    com.google.api.client.util.Data.nullOf(AuxiliaryNodeGroup.class);
  }

  /**
   * Optional. The type of the cluster.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.String clusterType;

  /**
   * Optional. A Cloud Storage bucket used to stage job dependencies, config files, and job driver
   * console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud
   * Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute
   * Engine zone where your cluster is deployed, and then create and manage this project-level, per-
   * location bucket (see Dataproc staging and temp buckets
   * (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This
   * field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.String configBucket;

  /**
   * Optional. The config for Dataproc metrics.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private DataprocMetricConfig dataprocMetricConfig;

  /**
   * Optional. Encryption settings for the cluster.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private EncryptionConfig encryptionConfig;

  /**
   * Optional. Port/endpoint configuration for this cluster
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private EndpointConfig endpointConfig;

  /**
   * Optional. The shared Compute Engine config settings for all instances in a cluster.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private GceClusterConfig gceClusterConfig;

  /**
   * Optional. BETA. The Kubernetes Engine config for Dataproc clusters deployed to The Kubernetes
   * Engine config for Dataproc clusters deployed to Kubernetes. These config settings are mutually
   * exclusive with Compute Engine-based options, such as gce_cluster_config, master_config,
   * worker_config, secondary_worker_config, and autoscaling_config.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private GkeClusterConfig gkeClusterConfig;

  /**
   * Optional. Commands to execute on each node after config is completed. By default, executables
   * are run on master and all worker nodes. You can test a node's role metadata to run an
   * executable on a master or worker node, as shown below using curl (you can also use wget):
   * ROLE=$(curl -H Metadata-Flavor:Google
   * http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if [[ "${ROLE}" ==
   * 'Master' ]]; then ... master specific actions ... else ... worker specific actions ... fi
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.util.List<NodeInitializationAction> initializationActions;

  /**
   * Optional. Lifecycle setting for the cluster.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private LifecycleConfig lifecycleConfig;

  /**
   * Optional. The Compute Engine config settings for the cluster's master instance.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private InstanceGroupConfig masterConfig;

  /**
   * Optional. Metastore configuration.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private MetastoreConfig metastoreConfig;

  /**
   * Optional. The Compute Engine config settings for a cluster's secondary worker instances
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private InstanceGroupConfig secondaryWorkerConfig;

  /**
   * Optional. Security settings for the cluster.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private SecurityConfig securityConfig;

  /**
   * Optional. The config settings for cluster software.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private SoftwareConfig softwareConfig;

  /**
   * Optional. A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark
   * and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a
   * Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the
   * Compute Engine zone where your cluster is deployed, and then create and manage this project-
   * level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL
   * (or none) if you specify a bucket (see Dataproc staging and temp buckets
   * (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This
   * field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.String tempBucket;

  /**
   * Optional. The Compute Engine config settings for the cluster's worker instances.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private InstanceGroupConfig workerConfig;

  /**
   * Optional. Autoscaling config for the policy associated with the cluster. Cluster does not
   * autoscale if this field is unset.
   * @return value or {@code null} for none
   */
  public AutoscalingConfig getAutoscalingConfig() {
    return autoscalingConfig;
  }

  /**
   * Optional. Autoscaling config for the policy associated with the cluster. Cluster does not
   * autoscale if this field is unset.
   * @param autoscalingConfig autoscalingConfig or {@code null} for none
   */
  public ClusterConfig setAutoscalingConfig(AutoscalingConfig autoscalingConfig) {
    this.autoscalingConfig = autoscalingConfig;
    return this;
  }

  /**
   * Optional. The node group settings.
   * @return value or {@code null} for none
   */
  public java.util.List<AuxiliaryNodeGroup> getAuxiliaryNodeGroups() {
    return auxiliaryNodeGroups;
  }

  /**
   * Optional. The node group settings.
   * @param auxiliaryNodeGroups auxiliaryNodeGroups or {@code null} for none
   */
  public ClusterConfig setAuxiliaryNodeGroups(java.util.List<AuxiliaryNodeGroup> auxiliaryNodeGroups) {
    this.auxiliaryNodeGroups = auxiliaryNodeGroups;
    return this;
  }

  /**
   * Optional. The type of the cluster.
   * @return value or {@code null} for none
   */
  public java.lang.String getClusterType() {
    return clusterType;
  }

  /**
   * Optional. The type of the cluster.
   * @param clusterType clusterType or {@code null} for none
   */
  public ClusterConfig setClusterType(java.lang.String clusterType) {
    this.clusterType = clusterType;
    return this;
  }

  /**
   * Optional. A Cloud Storage bucket used to stage job dependencies, config files, and job driver
   * console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud
   * Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute
   * Engine zone where your cluster is deployed, and then create and manage this project-level, per-
   * location bucket (see Dataproc staging and temp buckets
   * (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This
   * field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
   * @return value or {@code null} for none
   */
  public java.lang.String getConfigBucket() {
    return configBucket;
  }

  /**
   * Optional. A Cloud Storage bucket used to stage job dependencies, config files, and job driver
   * console output. If you do not specify a staging bucket, Cloud Dataproc will determine a Cloud
   * Storage location (US, ASIA, or EU) for your cluster's staging bucket according to the Compute
   * Engine zone where your cluster is deployed, and then create and manage this project-level, per-
   * location bucket (see Dataproc staging and temp buckets
   * (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This
   * field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
   * @param configBucket configBucket or {@code null} for none
   */
  public ClusterConfig setConfigBucket(java.lang.String configBucket) {
    this.configBucket = configBucket;
    return this;
  }

  /**
   * Optional. The config for Dataproc metrics.
   * @return value or {@code null} for none
   */
  public DataprocMetricConfig getDataprocMetricConfig() {
    return dataprocMetricConfig;
  }

  /**
   * Optional. The config for Dataproc metrics.
   * @param dataprocMetricConfig dataprocMetricConfig or {@code null} for none
   */
  public ClusterConfig setDataprocMetricConfig(DataprocMetricConfig dataprocMetricConfig) {
    this.dataprocMetricConfig = dataprocMetricConfig;
    return this;
  }

  /**
   * Optional. Encryption settings for the cluster.
   * @return value or {@code null} for none
   */
  public EncryptionConfig getEncryptionConfig() {
    return encryptionConfig;
  }

  /**
   * Optional. Encryption settings for the cluster.
   * @param encryptionConfig encryptionConfig or {@code null} for none
   */
  public ClusterConfig setEncryptionConfig(EncryptionConfig encryptionConfig) {
    this.encryptionConfig = encryptionConfig;
    return this;
  }

  /**
   * Optional. Port/endpoint configuration for this cluster
   * @return value or {@code null} for none
   */
  public EndpointConfig getEndpointConfig() {
    return endpointConfig;
  }

  /**
   * Optional. Port/endpoint configuration for this cluster
   * @param endpointConfig endpointConfig or {@code null} for none
   */
  public ClusterConfig setEndpointConfig(EndpointConfig endpointConfig) {
    this.endpointConfig = endpointConfig;
    return this;
  }

  /**
   * Optional. The shared Compute Engine config settings for all instances in a cluster.
   * @return value or {@code null} for none
   */
  public GceClusterConfig getGceClusterConfig() {
    return gceClusterConfig;
  }

  /**
   * Optional. The shared Compute Engine config settings for all instances in a cluster.
   * @param gceClusterConfig gceClusterConfig or {@code null} for none
   */
  public ClusterConfig setGceClusterConfig(GceClusterConfig gceClusterConfig) {
    this.gceClusterConfig = gceClusterConfig;
    return this;
  }

  /**
   * Optional. BETA. The Kubernetes Engine config for Dataproc clusters deployed to The Kubernetes
   * Engine config for Dataproc clusters deployed to Kubernetes. These config settings are mutually
   * exclusive with Compute Engine-based options, such as gce_cluster_config, master_config,
   * worker_config, secondary_worker_config, and autoscaling_config.
   * @return value or {@code null} for none
   */
  public GkeClusterConfig getGkeClusterConfig() {
    return gkeClusterConfig;
  }

  /**
   * Optional. BETA. The Kubernetes Engine config for Dataproc clusters deployed to The Kubernetes
   * Engine config for Dataproc clusters deployed to Kubernetes. These config settings are mutually
   * exclusive with Compute Engine-based options, such as gce_cluster_config, master_config,
   * worker_config, secondary_worker_config, and autoscaling_config.
   * @param gkeClusterConfig gkeClusterConfig or {@code null} for none
   */
  public ClusterConfig setGkeClusterConfig(GkeClusterConfig gkeClusterConfig) {
    this.gkeClusterConfig = gkeClusterConfig;
    return this;
  }

  /**
   * Optional. Commands to execute on each node after config is completed. By default, executables
   * are run on master and all worker nodes. You can test a node's role metadata to run an
   * executable on a master or worker node, as shown below using curl (you can also use wget):
   * ROLE=$(curl -H Metadata-Flavor:Google
   * http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if [[ "${ROLE}" ==
   * 'Master' ]]; then ... master specific actions ... else ... worker specific actions ... fi
   * @return value or {@code null} for none
   */
  public java.util.List<NodeInitializationAction> getInitializationActions() {
    return initializationActions;
  }

  /**
   * Optional. Commands to execute on each node after config is completed. By default, executables
   * are run on master and all worker nodes. You can test a node's role metadata to run an
   * executable on a master or worker node, as shown below using curl (you can also use wget):
   * ROLE=$(curl -H Metadata-Flavor:Google
   * http://metadata/computeMetadata/v1/instance/attributes/dataproc-role) if [[ "${ROLE}" ==
   * 'Master' ]]; then ... master specific actions ... else ... worker specific actions ... fi
   * @param initializationActions initializationActions or {@code null} for none
   */
  public ClusterConfig setInitializationActions(java.util.List<NodeInitializationAction> initializationActions) {
    this.initializationActions = initializationActions;
    return this;
  }

  /**
   * Optional. Lifecycle setting for the cluster.
   * @return value or {@code null} for none
   */
  public LifecycleConfig getLifecycleConfig() {
    return lifecycleConfig;
  }

  /**
   * Optional. Lifecycle setting for the cluster.
   * @param lifecycleConfig lifecycleConfig or {@code null} for none
   */
  public ClusterConfig setLifecycleConfig(LifecycleConfig lifecycleConfig) {
    this.lifecycleConfig = lifecycleConfig;
    return this;
  }

  /**
   * Optional. The Compute Engine config settings for the cluster's master instance.
   * @return value or {@code null} for none
   */
  public InstanceGroupConfig getMasterConfig() {
    return masterConfig;
  }

  /**
   * Optional. The Compute Engine config settings for the cluster's master instance.
   * @param masterConfig masterConfig or {@code null} for none
   */
  public ClusterConfig setMasterConfig(InstanceGroupConfig masterConfig) {
    this.masterConfig = masterConfig;
    return this;
  }

  /**
   * Optional. Metastore configuration.
   * @return value or {@code null} for none
   */
  public MetastoreConfig getMetastoreConfig() {
    return metastoreConfig;
  }

  /**
   * Optional. Metastore configuration.
   * @param metastoreConfig metastoreConfig or {@code null} for none
   */
  public ClusterConfig setMetastoreConfig(MetastoreConfig metastoreConfig) {
    this.metastoreConfig = metastoreConfig;
    return this;
  }

  /**
   * Optional. The Compute Engine config settings for a cluster's secondary worker instances
   * @return value or {@code null} for none
   */
  public InstanceGroupConfig getSecondaryWorkerConfig() {
    return secondaryWorkerConfig;
  }

  /**
   * Optional. The Compute Engine config settings for a cluster's secondary worker instances
   * @param secondaryWorkerConfig secondaryWorkerConfig or {@code null} for none
   */
  public ClusterConfig setSecondaryWorkerConfig(InstanceGroupConfig secondaryWorkerConfig) {
    this.secondaryWorkerConfig = secondaryWorkerConfig;
    return this;
  }

  /**
   * Optional. Security settings for the cluster.
   * @return value or {@code null} for none
   */
  public SecurityConfig getSecurityConfig() {
    return securityConfig;
  }

  /**
   * Optional. Security settings for the cluster.
   * @param securityConfig securityConfig or {@code null} for none
   */
  public ClusterConfig setSecurityConfig(SecurityConfig securityConfig) {
    this.securityConfig = securityConfig;
    return this;
  }

  /**
   * Optional. The config settings for cluster software.
   * @return value or {@code null} for none
   */
  public SoftwareConfig getSoftwareConfig() {
    return softwareConfig;
  }

  /**
   * Optional. The config settings for cluster software.
   * @param softwareConfig softwareConfig or {@code null} for none
   */
  public ClusterConfig setSoftwareConfig(SoftwareConfig softwareConfig) {
    this.softwareConfig = softwareConfig;
    return this;
  }

  /**
   * Optional. A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark
   * and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a
   * Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the
   * Compute Engine zone where your cluster is deployed, and then create and manage this project-
   * level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL
   * (or none) if you specify a bucket (see Dataproc staging and temp buckets
   * (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This
   * field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
   * @return value or {@code null} for none
   */
  public java.lang.String getTempBucket() {
    return tempBucket;
  }

  /**
   * Optional. A Cloud Storage bucket used to store ephemeral cluster and jobs data, such as Spark
   * and MapReduce history files. If you do not specify a temp bucket, Dataproc will determine a
   * Cloud Storage location (US, ASIA, or EU) for your cluster's temp bucket according to the
   * Compute Engine zone where your cluster is deployed, and then create and manage this project-
   * level, per-location bucket. The default bucket has a TTL of 90 days, but you can use any TTL
   * (or none) if you specify a bucket (see Dataproc staging and temp buckets
   * (https://cloud.google.com/dataproc/docs/concepts/configuring-clusters/staging-bucket)). This
   * field requires a Cloud Storage bucket name, not a gs://... URI to a Cloud Storage bucket.
   * @param tempBucket tempBucket or {@code null} for none
   */
  public ClusterConfig setTempBucket(java.lang.String tempBucket) {
    this.tempBucket = tempBucket;
    return this;
  }

  /**
   * Optional. The Compute Engine config settings for the cluster's worker instances.
   * @return value or {@code null} for none
   */
  public InstanceGroupConfig getWorkerConfig() {
    return workerConfig;
  }

  /**
   * Optional. The Compute Engine config settings for the cluster's worker instances.
   * @param workerConfig workerConfig or {@code null} for none
   */
  public ClusterConfig setWorkerConfig(InstanceGroupConfig workerConfig) {
    this.workerConfig = workerConfig;
    return this;
  }

  @Override
  public ClusterConfig set(String fieldName, Object value) {
    return (ClusterConfig) super.set(fieldName, value);
  }

  @Override
  public ClusterConfig clone() {
    return (ClusterConfig) super.clone();
  }

}
