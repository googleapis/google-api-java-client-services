/*
 * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
 * in compliance with the License. You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License
 * is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
 * or implied. See the License for the specific language governing permissions and limitations under
 * the License.
 */
/*
 * This code was generated by https://github.com/googleapis/google-api-java-client-services/
 * Modify at your own risk.
 */

package com.google.api.services.discoveryengine.v1.model;

/**
 * BigQuery source import data from.
 *
 * <p> This is the Java data model class that specifies how to parse/serialize into the JSON that is
 * transmitted over HTTP when working with the Discovery Engine API. For a detailed explanation see:
 * <a href="https://developers.google.com/api-client-library/java/google-http-java-client/json">https://developers.google.com/api-client-library/java/google-http-java-client/json</a>
 * </p>
 *
 * @author Google, Inc.
 */
@SuppressWarnings("javadoc")
public final class GoogleCloudDiscoveryengineV1BigQuerySource extends com.google.api.client.json.GenericJson {

  /**
   * The schema to use when parsing the data from the source. Supported values for user event
   * imports: * `user_event` (default): One UserEvent per row. Supported values for document
   * imports: * `document` (default): One Document format per row. Each document must have a valid
   * Document.id and one of Document.json_data or Document.struct_data. * `custom`: One custom data
   * per row in arbitrary format that conforms to the defined Schema of the data store. This can
   * only be used by the GENERIC Data Store vertical.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.String dataSchema;

  /**
   * Required. The BigQuery data set to copy the data from with a length limit of 1,024 characters.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.String datasetId;

  /**
   * Intermediate Cloud Storage directory used for the import with a length limit of 2,000
   * characters. Can be specified if one wants to have the BigQuery export to a specific Cloud
   * Storage directory.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.String gcsStagingDir;

  /**
   * BigQuery time partitioned table's _PARTITIONDATE in YYYY-MM-DD format.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private GoogleTypeDate partitionDate;

  /**
   * The project ID (can be project # or ID) that the BigQuery source is in with a length limit of
   * 128 characters. If not specified, inherits the project ID from the parent request.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.String projectId;

  /**
   * Required. The BigQuery table to copy the data from with a length limit of 1,024 characters.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.String tableId;

  /**
   * The schema to use when parsing the data from the source. Supported values for user event
   * imports: * `user_event` (default): One UserEvent per row. Supported values for document
   * imports: * `document` (default): One Document format per row. Each document must have a valid
   * Document.id and one of Document.json_data or Document.struct_data. * `custom`: One custom data
   * per row in arbitrary format that conforms to the defined Schema of the data store. This can
   * only be used by the GENERIC Data Store vertical.
   * @return value or {@code null} for none
   */
  public java.lang.String getDataSchema() {
    return dataSchema;
  }

  /**
   * The schema to use when parsing the data from the source. Supported values for user event
   * imports: * `user_event` (default): One UserEvent per row. Supported values for document
   * imports: * `document` (default): One Document format per row. Each document must have a valid
   * Document.id and one of Document.json_data or Document.struct_data. * `custom`: One custom data
   * per row in arbitrary format that conforms to the defined Schema of the data store. This can
   * only be used by the GENERIC Data Store vertical.
   * @param dataSchema dataSchema or {@code null} for none
   */
  public GoogleCloudDiscoveryengineV1BigQuerySource setDataSchema(java.lang.String dataSchema) {
    this.dataSchema = dataSchema;
    return this;
  }

  /**
   * Required. The BigQuery data set to copy the data from with a length limit of 1,024 characters.
   * @return value or {@code null} for none
   */
  public java.lang.String getDatasetId() {
    return datasetId;
  }

  /**
   * Required. The BigQuery data set to copy the data from with a length limit of 1,024 characters.
   * @param datasetId datasetId or {@code null} for none
   */
  public GoogleCloudDiscoveryengineV1BigQuerySource setDatasetId(java.lang.String datasetId) {
    this.datasetId = datasetId;
    return this;
  }

  /**
   * Intermediate Cloud Storage directory used for the import with a length limit of 2,000
   * characters. Can be specified if one wants to have the BigQuery export to a specific Cloud
   * Storage directory.
   * @return value or {@code null} for none
   */
  public java.lang.String getGcsStagingDir() {
    return gcsStagingDir;
  }

  /**
   * Intermediate Cloud Storage directory used for the import with a length limit of 2,000
   * characters. Can be specified if one wants to have the BigQuery export to a specific Cloud
   * Storage directory.
   * @param gcsStagingDir gcsStagingDir or {@code null} for none
   */
  public GoogleCloudDiscoveryengineV1BigQuerySource setGcsStagingDir(java.lang.String gcsStagingDir) {
    this.gcsStagingDir = gcsStagingDir;
    return this;
  }

  /**
   * BigQuery time partitioned table's _PARTITIONDATE in YYYY-MM-DD format.
   * @return value or {@code null} for none
   */
  public GoogleTypeDate getPartitionDate() {
    return partitionDate;
  }

  /**
   * BigQuery time partitioned table's _PARTITIONDATE in YYYY-MM-DD format.
   * @param partitionDate partitionDate or {@code null} for none
   */
  public GoogleCloudDiscoveryengineV1BigQuerySource setPartitionDate(GoogleTypeDate partitionDate) {
    this.partitionDate = partitionDate;
    return this;
  }

  /**
   * The project ID (can be project # or ID) that the BigQuery source is in with a length limit of
   * 128 characters. If not specified, inherits the project ID from the parent request.
   * @return value or {@code null} for none
   */
  public java.lang.String getProjectId() {
    return projectId;
  }

  /**
   * The project ID (can be project # or ID) that the BigQuery source is in with a length limit of
   * 128 characters. If not specified, inherits the project ID from the parent request.
   * @param projectId projectId or {@code null} for none
   */
  public GoogleCloudDiscoveryengineV1BigQuerySource setProjectId(java.lang.String projectId) {
    this.projectId = projectId;
    return this;
  }

  /**
   * Required. The BigQuery table to copy the data from with a length limit of 1,024 characters.
   * @return value or {@code null} for none
   */
  public java.lang.String getTableId() {
    return tableId;
  }

  /**
   * Required. The BigQuery table to copy the data from with a length limit of 1,024 characters.
   * @param tableId tableId or {@code null} for none
   */
  public GoogleCloudDiscoveryengineV1BigQuerySource setTableId(java.lang.String tableId) {
    this.tableId = tableId;
    return this;
  }

  @Override
  public GoogleCloudDiscoveryengineV1BigQuerySource set(String fieldName, Object value) {
    return (GoogleCloudDiscoveryengineV1BigQuerySource) super.set(fieldName, value);
  }

  @Override
  public GoogleCloudDiscoveryengineV1BigQuerySource clone() {
    return (GoogleCloudDiscoveryengineV1BigQuerySource) super.clone();
  }

}
