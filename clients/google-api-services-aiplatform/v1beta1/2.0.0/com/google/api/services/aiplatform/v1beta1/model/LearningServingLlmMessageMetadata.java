/*
 * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
 * in compliance with the License. You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License
 * is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
 * or implied. See the License for the specific language governing permissions and limitations under
 * the License.
 */
/*
 * This code was generated by https://github.com/googleapis/google-api-java-client-services/
 * Modify at your own risk.
 */

package com.google.api.services.aiplatform.v1beta1.model;

/**
 * LINT.IfChange This metadata contains additional information required for debugging.
 *
 * <p> This is the Java data model class that specifies how to parse/serialize into the JSON that is
 * transmitted over HTTP when working with the Vertex AI API. For a detailed explanation see:
 * <a href="https://developers.google.com/api-client-library/java/google-http-java-client/json">https://developers.google.com/api-client-library/java/google-http-java-client/json</a>
 * </p>
 *
 * @author Google, Inc.
 */
@SuppressWarnings("javadoc")
public final class LearningServingLlmMessageMetadata extends com.google.api.client.json.GenericJson {

  /**
   * Summary of classifier output. We attach this to all messages regardless of whether
   * classification rules triggered or not.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private LearningGenaiRootClassifierOutputSummary classifierSummary;

  /**
   * Contains metadata related to Codey Processors.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private LearningGenaiRootCodeyOutput codeyOutput;

  /**
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.Long currentStreamTextLength;

  /**
   * Whether the corresponding message has been deleted.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.Boolean deleted;

  /**
   * Metadata for filters that triggered.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.util.List<LearningGenaiRootFilterMetadata> filterMeta;

  static {
    // hack to force ProGuard to consider LearningGenaiRootFilterMetadata used, since otherwise it would be stripped out
    // see https://github.com/google/google-api-java-client/issues/543
    com.google.api.client.util.Data.nullOf(LearningGenaiRootFilterMetadata.class);
  }

  /**
   * This score is finally used for ranking the message. This will be same as the score present in
   * `Message.score` field.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private LearningGenaiRootScore finalMessageScore;

  /**
   * NOT YET IMPLEMENTED.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.String finishReason;

  /**
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private LearningGenaiRootGroundingMetadata groundingMetadata;

  /**
   * Applies to streaming response message only. Whether the message is a code.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.Boolean isCode;

  /**
   * Applies to Response message only. Indicates whether the message is a fallback and the response
   * would have otherwise been empty.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.Boolean isFallback;

  /**
   * Result from nlp_saft DetectLanguage method. Currently the predicted language code and language
   * probability is used.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private NlpSaftLangIdResult langidResult;

  /**
   * Detected language.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.String language;

  /**
   * The LM prefix used to generate this response.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.String lmPrefix;

  /**
   * The original text generated by LLM. This is the raw output for debugging purposes.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.String originalText;

  /**
   * NOT YET IMPLEMENTED. Applies to streaming only. Number of tokens decoded / emitted by the model
   * as part of this stream. This may be different from token_count, which contains number of tokens
   * returned in this response after any response rewriting / truncation.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.Integer perStreamDecodedTokenCount;

  /**
   * Results of running RAI on the query or this response candidate. One output per rai_config. It
   * will be populated regardless of whether the threshold is exceeded or not.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.util.List<LearningGenaiRootRAIOutput> raiOutputs;

  static {
    // hack to force ProGuard to consider LearningGenaiRootRAIOutput used, since otherwise it would be stripped out
    // see https://github.com/google/google-api-java-client/issues/543
    com.google.api.client.util.Data.nullOf(LearningGenaiRootRAIOutput.class);
  }

  /**
   * Recitation Results. It will be populated as long as Recitation processing is enabled,
   * regardless of recitation outcome.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private LearningGenaiRecitationRecitationResult recitationResult;

  /**
   * NOT YET IMPLEMENTED. Number of tokens returned as part of this candidate.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.Integer returnTokenCount;

  /**
   * All the different scores for a message are logged here.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.util.List<LearningGenaiRootScore> scores;

  static {
    // hack to force ProGuard to consider LearningGenaiRootScore used, since otherwise it would be stripped out
    // see https://github.com/google/google-api-java-client/issues/543
    com.google.api.client.util.Data.nullOf(LearningGenaiRootScore.class);
  }

  /**
   * Whether the response is terminated during streaming return. Only used for streaming requests.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.Boolean streamTerminated;

  /**
   * NOT YET IMPLEMENTED. Aggregated number of total tokens decoded so far. For streaming, this is
   * sum of all the tokens decoded so far i.e. aggregated count.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.Integer totalDecodedTokenCount;

  /**
   * Translated user-prompt used for RAI post processing. This is for internal processing only. We
   * will translate in pre-processor and pass the translated text to the post processor using this
   * field. It will be empty if non of the signals requested need translation.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.util.List<java.lang.String> translatedUserPrompts;

  /**
   * The metadata from Vertex SafetyCat processors
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private CloudAiNlLlmProtoServiceRaiResult vertexRaiResult;

  /**
   * Summary of classifier output. We attach this to all messages regardless of whether
   * classification rules triggered or not.
   * @return value or {@code null} for none
   */
  public LearningGenaiRootClassifierOutputSummary getClassifierSummary() {
    return classifierSummary;
  }

  /**
   * Summary of classifier output. We attach this to all messages regardless of whether
   * classification rules triggered or not.
   * @param classifierSummary classifierSummary or {@code null} for none
   */
  public LearningServingLlmMessageMetadata setClassifierSummary(LearningGenaiRootClassifierOutputSummary classifierSummary) {
    this.classifierSummary = classifierSummary;
    return this;
  }

  /**
   * Contains metadata related to Codey Processors.
   * @return value or {@code null} for none
   */
  public LearningGenaiRootCodeyOutput getCodeyOutput() {
    return codeyOutput;
  }

  /**
   * Contains metadata related to Codey Processors.
   * @param codeyOutput codeyOutput or {@code null} for none
   */
  public LearningServingLlmMessageMetadata setCodeyOutput(LearningGenaiRootCodeyOutput codeyOutput) {
    this.codeyOutput = codeyOutput;
    return this;
  }

  /**
   * @return value or {@code null} for none
   */
  public java.lang.Long getCurrentStreamTextLength() {
    return currentStreamTextLength;
  }

  /**
   * @param currentStreamTextLength currentStreamTextLength or {@code null} for none
   */
  public LearningServingLlmMessageMetadata setCurrentStreamTextLength(java.lang.Long currentStreamTextLength) {
    this.currentStreamTextLength = currentStreamTextLength;
    return this;
  }

  /**
   * Whether the corresponding message has been deleted.
   * @return value or {@code null} for none
   */
  public java.lang.Boolean getDeleted() {
    return deleted;
  }

  /**
   * Whether the corresponding message has been deleted.
   * @param deleted deleted or {@code null} for none
   */
  public LearningServingLlmMessageMetadata setDeleted(java.lang.Boolean deleted) {
    this.deleted = deleted;
    return this;
  }

  /**
   * Metadata for filters that triggered.
   * @return value or {@code null} for none
   */
  public java.util.List<LearningGenaiRootFilterMetadata> getFilterMeta() {
    return filterMeta;
  }

  /**
   * Metadata for filters that triggered.
   * @param filterMeta filterMeta or {@code null} for none
   */
  public LearningServingLlmMessageMetadata setFilterMeta(java.util.List<LearningGenaiRootFilterMetadata> filterMeta) {
    this.filterMeta = filterMeta;
    return this;
  }

  /**
   * This score is finally used for ranking the message. This will be same as the score present in
   * `Message.score` field.
   * @return value or {@code null} for none
   */
  public LearningGenaiRootScore getFinalMessageScore() {
    return finalMessageScore;
  }

  /**
   * This score is finally used for ranking the message. This will be same as the score present in
   * `Message.score` field.
   * @param finalMessageScore finalMessageScore or {@code null} for none
   */
  public LearningServingLlmMessageMetadata setFinalMessageScore(LearningGenaiRootScore finalMessageScore) {
    this.finalMessageScore = finalMessageScore;
    return this;
  }

  /**
   * NOT YET IMPLEMENTED.
   * @return value or {@code null} for none
   */
  public java.lang.String getFinishReason() {
    return finishReason;
  }

  /**
   * NOT YET IMPLEMENTED.
   * @param finishReason finishReason or {@code null} for none
   */
  public LearningServingLlmMessageMetadata setFinishReason(java.lang.String finishReason) {
    this.finishReason = finishReason;
    return this;
  }

  /**
   * @return value or {@code null} for none
   */
  public LearningGenaiRootGroundingMetadata getGroundingMetadata() {
    return groundingMetadata;
  }

  /**
   * @param groundingMetadata groundingMetadata or {@code null} for none
   */
  public LearningServingLlmMessageMetadata setGroundingMetadata(LearningGenaiRootGroundingMetadata groundingMetadata) {
    this.groundingMetadata = groundingMetadata;
    return this;
  }

  /**
   * Applies to streaming response message only. Whether the message is a code.
   * @return value or {@code null} for none
   */
  public java.lang.Boolean getIsCode() {
    return isCode;
  }

  /**
   * Applies to streaming response message only. Whether the message is a code.
   * @param isCode isCode or {@code null} for none
   */
  public LearningServingLlmMessageMetadata setIsCode(java.lang.Boolean isCode) {
    this.isCode = isCode;
    return this;
  }

  /**
   * Applies to Response message only. Indicates whether the message is a fallback and the response
   * would have otherwise been empty.
   * @return value or {@code null} for none
   */
  public java.lang.Boolean getIsFallback() {
    return isFallback;
  }

  /**
   * Applies to Response message only. Indicates whether the message is a fallback and the response
   * would have otherwise been empty.
   * @param isFallback isFallback or {@code null} for none
   */
  public LearningServingLlmMessageMetadata setIsFallback(java.lang.Boolean isFallback) {
    this.isFallback = isFallback;
    return this;
  }

  /**
   * Result from nlp_saft DetectLanguage method. Currently the predicted language code and language
   * probability is used.
   * @return value or {@code null} for none
   */
  public NlpSaftLangIdResult getLangidResult() {
    return langidResult;
  }

  /**
   * Result from nlp_saft DetectLanguage method. Currently the predicted language code and language
   * probability is used.
   * @param langidResult langidResult or {@code null} for none
   */
  public LearningServingLlmMessageMetadata setLangidResult(NlpSaftLangIdResult langidResult) {
    this.langidResult = langidResult;
    return this;
  }

  /**
   * Detected language.
   * @return value or {@code null} for none
   */
  public java.lang.String getLanguage() {
    return language;
  }

  /**
   * Detected language.
   * @param language language or {@code null} for none
   */
  public LearningServingLlmMessageMetadata setLanguage(java.lang.String language) {
    this.language = language;
    return this;
  }

  /**
   * The LM prefix used to generate this response.
   * @return value or {@code null} for none
   */
  public java.lang.String getLmPrefix() {
    return lmPrefix;
  }

  /**
   * The LM prefix used to generate this response.
   * @param lmPrefix lmPrefix or {@code null} for none
   */
  public LearningServingLlmMessageMetadata setLmPrefix(java.lang.String lmPrefix) {
    this.lmPrefix = lmPrefix;
    return this;
  }

  /**
   * The original text generated by LLM. This is the raw output for debugging purposes.
   * @return value or {@code null} for none
   */
  public java.lang.String getOriginalText() {
    return originalText;
  }

  /**
   * The original text generated by LLM. This is the raw output for debugging purposes.
   * @param originalText originalText or {@code null} for none
   */
  public LearningServingLlmMessageMetadata setOriginalText(java.lang.String originalText) {
    this.originalText = originalText;
    return this;
  }

  /**
   * NOT YET IMPLEMENTED. Applies to streaming only. Number of tokens decoded / emitted by the model
   * as part of this stream. This may be different from token_count, which contains number of tokens
   * returned in this response after any response rewriting / truncation.
   * @return value or {@code null} for none
   */
  public java.lang.Integer getPerStreamDecodedTokenCount() {
    return perStreamDecodedTokenCount;
  }

  /**
   * NOT YET IMPLEMENTED. Applies to streaming only. Number of tokens decoded / emitted by the model
   * as part of this stream. This may be different from token_count, which contains number of tokens
   * returned in this response after any response rewriting / truncation.
   * @param perStreamDecodedTokenCount perStreamDecodedTokenCount or {@code null} for none
   */
  public LearningServingLlmMessageMetadata setPerStreamDecodedTokenCount(java.lang.Integer perStreamDecodedTokenCount) {
    this.perStreamDecodedTokenCount = perStreamDecodedTokenCount;
    return this;
  }

  /**
   * Results of running RAI on the query or this response candidate. One output per rai_config. It
   * will be populated regardless of whether the threshold is exceeded or not.
   * @return value or {@code null} for none
   */
  public java.util.List<LearningGenaiRootRAIOutput> getRaiOutputs() {
    return raiOutputs;
  }

  /**
   * Results of running RAI on the query or this response candidate. One output per rai_config. It
   * will be populated regardless of whether the threshold is exceeded or not.
   * @param raiOutputs raiOutputs or {@code null} for none
   */
  public LearningServingLlmMessageMetadata setRaiOutputs(java.util.List<LearningGenaiRootRAIOutput> raiOutputs) {
    this.raiOutputs = raiOutputs;
    return this;
  }

  /**
   * Recitation Results. It will be populated as long as Recitation processing is enabled,
   * regardless of recitation outcome.
   * @return value or {@code null} for none
   */
  public LearningGenaiRecitationRecitationResult getRecitationResult() {
    return recitationResult;
  }

  /**
   * Recitation Results. It will be populated as long as Recitation processing is enabled,
   * regardless of recitation outcome.
   * @param recitationResult recitationResult or {@code null} for none
   */
  public LearningServingLlmMessageMetadata setRecitationResult(LearningGenaiRecitationRecitationResult recitationResult) {
    this.recitationResult = recitationResult;
    return this;
  }

  /**
   * NOT YET IMPLEMENTED. Number of tokens returned as part of this candidate.
   * @return value or {@code null} for none
   */
  public java.lang.Integer getReturnTokenCount() {
    return returnTokenCount;
  }

  /**
   * NOT YET IMPLEMENTED. Number of tokens returned as part of this candidate.
   * @param returnTokenCount returnTokenCount or {@code null} for none
   */
  public LearningServingLlmMessageMetadata setReturnTokenCount(java.lang.Integer returnTokenCount) {
    this.returnTokenCount = returnTokenCount;
    return this;
  }

  /**
   * All the different scores for a message are logged here.
   * @return value or {@code null} for none
   */
  public java.util.List<LearningGenaiRootScore> getScores() {
    return scores;
  }

  /**
   * All the different scores for a message are logged here.
   * @param scores scores or {@code null} for none
   */
  public LearningServingLlmMessageMetadata setScores(java.util.List<LearningGenaiRootScore> scores) {
    this.scores = scores;
    return this;
  }

  /**
   * Whether the response is terminated during streaming return. Only used for streaming requests.
   * @return value or {@code null} for none
   */
  public java.lang.Boolean getStreamTerminated() {
    return streamTerminated;
  }

  /**
   * Whether the response is terminated during streaming return. Only used for streaming requests.
   * @param streamTerminated streamTerminated or {@code null} for none
   */
  public LearningServingLlmMessageMetadata setStreamTerminated(java.lang.Boolean streamTerminated) {
    this.streamTerminated = streamTerminated;
    return this;
  }

  /**
   * NOT YET IMPLEMENTED. Aggregated number of total tokens decoded so far. For streaming, this is
   * sum of all the tokens decoded so far i.e. aggregated count.
   * @return value or {@code null} for none
   */
  public java.lang.Integer getTotalDecodedTokenCount() {
    return totalDecodedTokenCount;
  }

  /**
   * NOT YET IMPLEMENTED. Aggregated number of total tokens decoded so far. For streaming, this is
   * sum of all the tokens decoded so far i.e. aggregated count.
   * @param totalDecodedTokenCount totalDecodedTokenCount or {@code null} for none
   */
  public LearningServingLlmMessageMetadata setTotalDecodedTokenCount(java.lang.Integer totalDecodedTokenCount) {
    this.totalDecodedTokenCount = totalDecodedTokenCount;
    return this;
  }

  /**
   * Translated user-prompt used for RAI post processing. This is for internal processing only. We
   * will translate in pre-processor and pass the translated text to the post processor using this
   * field. It will be empty if non of the signals requested need translation.
   * @return value or {@code null} for none
   */
  public java.util.List<java.lang.String> getTranslatedUserPrompts() {
    return translatedUserPrompts;
  }

  /**
   * Translated user-prompt used for RAI post processing. This is for internal processing only. We
   * will translate in pre-processor and pass the translated text to the post processor using this
   * field. It will be empty if non of the signals requested need translation.
   * @param translatedUserPrompts translatedUserPrompts or {@code null} for none
   */
  public LearningServingLlmMessageMetadata setTranslatedUserPrompts(java.util.List<java.lang.String> translatedUserPrompts) {
    this.translatedUserPrompts = translatedUserPrompts;
    return this;
  }

  /**
   * The metadata from Vertex SafetyCat processors
   * @return value or {@code null} for none
   */
  public CloudAiNlLlmProtoServiceRaiResult getVertexRaiResult() {
    return vertexRaiResult;
  }

  /**
   * The metadata from Vertex SafetyCat processors
   * @param vertexRaiResult vertexRaiResult or {@code null} for none
   */
  public LearningServingLlmMessageMetadata setVertexRaiResult(CloudAiNlLlmProtoServiceRaiResult vertexRaiResult) {
    this.vertexRaiResult = vertexRaiResult;
    return this;
  }

  @Override
  public LearningServingLlmMessageMetadata set(String fieldName, Object value) {
    return (LearningServingLlmMessageMetadata) super.set(fieldName, value);
  }

  @Override
  public LearningServingLlmMessageMetadata clone() {
    return (LearningServingLlmMessageMetadata) super.clone();
  }

}
