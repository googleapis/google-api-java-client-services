/*
 * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
 * in compliance with the License. You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License
 * is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
 * or implied. See the License for the specific language governing permissions and limitations under
 * the License.
 */
/*
 * This code was generated by https://github.com/googleapis/google-api-java-client-services/
 * Modify at your own risk.
 */

package com.google.api.services.aiplatform.v1beta1.model;

/**
 * Config for importing RagFiles.
 *
 * <p> This is the Java data model class that specifies how to parse/serialize into the JSON that is
 * transmitted over HTTP when working with the Vertex AI API. For a detailed explanation see:
 * <a href="https://developers.google.com/api-client-library/java/google-http-java-client/json">https://developers.google.com/api-client-library/java/google-http-java-client/json</a>
 * </p>
 *
 * @author Google, Inc.
 */
@SuppressWarnings("javadoc")
public final class GoogleCloudAiplatformV1beta1ImportRagFilesConfig extends com.google.api.client.json.GenericJson {

  /**
   * Google Cloud Storage location. Supports importing individual files as well as entire Google
   * Cloud Storage directories. Sample formats: -
   * `gs://bucket_name/my_directory/object_name/my_file.txt` - `gs://bucket_name/my_directory`
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private GoogleCloudAiplatformV1beta1GcsSource gcsSource;

  /**
   * Google Drive location. Supports importing individual files as well as Google Drive folders.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private GoogleCloudAiplatformV1beta1GoogleDriveSource googleDriveSource;

  /**
   * The BigQuery destination to write import result to. It should be a bigquery table resource name
   * (e.g. "bq://projectId.bqDatasetId.bqTableId"). The dataset must exist. If the table does not
   * exist, it will be created with the expected schema. If the table exists, the schema will be
   * validated and data will be added to this existing table.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private GoogleCloudAiplatformV1beta1BigQueryDestination importResultBigquerySink;

  /**
   * The Cloud Storage path to write import result to.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private GoogleCloudAiplatformV1beta1GcsDestination importResultGcsSink;

  /**
   * Jira queries with their corresponding authentication.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private GoogleCloudAiplatformV1beta1JiraSource jiraSource;

  /**
   * Optional. The max number of queries per minute that this job is allowed to make to the
   * embedding model specified on the corpus. This value is specific to this job and not shared
   * across other import jobs. Consult the Quotas page on the project to set an appropriate value
   * here. If unspecified, a default value of 1,000 QPM would be used.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.Integer maxEmbeddingRequestsPerMin;

  /**
   * The BigQuery destination to write partial failures to. It should be a bigquery table resource
   * name (e.g. "bq://projectId.bqDatasetId.bqTableId"). The dataset must exist. If the table does
   * not exist, it will be created with the expected schema. If the table exists, the schema will be
   * validated and data will be added to this existing table. Deprecated. Prefer to use
   * `import_result_bq_sink`.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private GoogleCloudAiplatformV1beta1BigQueryDestination partialFailureBigquerySink;

  /**
   * The Cloud Storage path to write partial failures to. Deprecated. Prefer to use
   * `import_result_gcs_sink`.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private GoogleCloudAiplatformV1beta1GcsDestination partialFailureGcsSink;

  /**
   * Specifies the size and overlap of chunks after importing RagFiles.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private GoogleCloudAiplatformV1beta1RagFileChunkingConfig ragFileChunkingConfig;

  /**
   * Optional. Specifies the parsing config for RagFiles. RAG will use the default parser if this
   * field is not set.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private GoogleCloudAiplatformV1beta1RagFileParsingConfig ragFileParsingConfig;

  /**
   * Specifies the transformation config for RagFiles.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private GoogleCloudAiplatformV1beta1RagFileTransformationConfig ragFileTransformationConfig;

  /**
   * SharePoint sources.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private GoogleCloudAiplatformV1beta1SharePointSources sharePointSources;

  /**
   * Slack channels with their corresponding access tokens.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private GoogleCloudAiplatformV1beta1SlackSource slackSource;

  /**
   * Google Cloud Storage location. Supports importing individual files as well as entire Google
   * Cloud Storage directories. Sample formats: -
   * `gs://bucket_name/my_directory/object_name/my_file.txt` - `gs://bucket_name/my_directory`
   * @return value or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GcsSource getGcsSource() {
    return gcsSource;
  }

  /**
   * Google Cloud Storage location. Supports importing individual files as well as entire Google
   * Cloud Storage directories. Sample formats: -
   * `gs://bucket_name/my_directory/object_name/my_file.txt` - `gs://bucket_name/my_directory`
   * @param gcsSource gcsSource or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1ImportRagFilesConfig setGcsSource(GoogleCloudAiplatformV1beta1GcsSource gcsSource) {
    this.gcsSource = gcsSource;
    return this;
  }

  /**
   * Google Drive location. Supports importing individual files as well as Google Drive folders.
   * @return value or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GoogleDriveSource getGoogleDriveSource() {
    return googleDriveSource;
  }

  /**
   * Google Drive location. Supports importing individual files as well as Google Drive folders.
   * @param googleDriveSource googleDriveSource or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1ImportRagFilesConfig setGoogleDriveSource(GoogleCloudAiplatformV1beta1GoogleDriveSource googleDriveSource) {
    this.googleDriveSource = googleDriveSource;
    return this;
  }

  /**
   * The BigQuery destination to write import result to. It should be a bigquery table resource name
   * (e.g. "bq://projectId.bqDatasetId.bqTableId"). The dataset must exist. If the table does not
   * exist, it will be created with the expected schema. If the table exists, the schema will be
   * validated and data will be added to this existing table.
   * @return value or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1BigQueryDestination getImportResultBigquerySink() {
    return importResultBigquerySink;
  }

  /**
   * The BigQuery destination to write import result to. It should be a bigquery table resource name
   * (e.g. "bq://projectId.bqDatasetId.bqTableId"). The dataset must exist. If the table does not
   * exist, it will be created with the expected schema. If the table exists, the schema will be
   * validated and data will be added to this existing table.
   * @param importResultBigquerySink importResultBigquerySink or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1ImportRagFilesConfig setImportResultBigquerySink(GoogleCloudAiplatformV1beta1BigQueryDestination importResultBigquerySink) {
    this.importResultBigquerySink = importResultBigquerySink;
    return this;
  }

  /**
   * The Cloud Storage path to write import result to.
   * @return value or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GcsDestination getImportResultGcsSink() {
    return importResultGcsSink;
  }

  /**
   * The Cloud Storage path to write import result to.
   * @param importResultGcsSink importResultGcsSink or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1ImportRagFilesConfig setImportResultGcsSink(GoogleCloudAiplatformV1beta1GcsDestination importResultGcsSink) {
    this.importResultGcsSink = importResultGcsSink;
    return this;
  }

  /**
   * Jira queries with their corresponding authentication.
   * @return value or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1JiraSource getJiraSource() {
    return jiraSource;
  }

  /**
   * Jira queries with their corresponding authentication.
   * @param jiraSource jiraSource or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1ImportRagFilesConfig setJiraSource(GoogleCloudAiplatformV1beta1JiraSource jiraSource) {
    this.jiraSource = jiraSource;
    return this;
  }

  /**
   * Optional. The max number of queries per minute that this job is allowed to make to the
   * embedding model specified on the corpus. This value is specific to this job and not shared
   * across other import jobs. Consult the Quotas page on the project to set an appropriate value
   * here. If unspecified, a default value of 1,000 QPM would be used.
   * @return value or {@code null} for none
   */
  public java.lang.Integer getMaxEmbeddingRequestsPerMin() {
    return maxEmbeddingRequestsPerMin;
  }

  /**
   * Optional. The max number of queries per minute that this job is allowed to make to the
   * embedding model specified on the corpus. This value is specific to this job and not shared
   * across other import jobs. Consult the Quotas page on the project to set an appropriate value
   * here. If unspecified, a default value of 1,000 QPM would be used.
   * @param maxEmbeddingRequestsPerMin maxEmbeddingRequestsPerMin or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1ImportRagFilesConfig setMaxEmbeddingRequestsPerMin(java.lang.Integer maxEmbeddingRequestsPerMin) {
    this.maxEmbeddingRequestsPerMin = maxEmbeddingRequestsPerMin;
    return this;
  }

  /**
   * The BigQuery destination to write partial failures to. It should be a bigquery table resource
   * name (e.g. "bq://projectId.bqDatasetId.bqTableId"). The dataset must exist. If the table does
   * not exist, it will be created with the expected schema. If the table exists, the schema will be
   * validated and data will be added to this existing table. Deprecated. Prefer to use
   * `import_result_bq_sink`.
   * @return value or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1BigQueryDestination getPartialFailureBigquerySink() {
    return partialFailureBigquerySink;
  }

  /**
   * The BigQuery destination to write partial failures to. It should be a bigquery table resource
   * name (e.g. "bq://projectId.bqDatasetId.bqTableId"). The dataset must exist. If the table does
   * not exist, it will be created with the expected schema. If the table exists, the schema will be
   * validated and data will be added to this existing table. Deprecated. Prefer to use
   * `import_result_bq_sink`.
   * @param partialFailureBigquerySink partialFailureBigquerySink or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1ImportRagFilesConfig setPartialFailureBigquerySink(GoogleCloudAiplatformV1beta1BigQueryDestination partialFailureBigquerySink) {
    this.partialFailureBigquerySink = partialFailureBigquerySink;
    return this;
  }

  /**
   * The Cloud Storage path to write partial failures to. Deprecated. Prefer to use
   * `import_result_gcs_sink`.
   * @return value or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GcsDestination getPartialFailureGcsSink() {
    return partialFailureGcsSink;
  }

  /**
   * The Cloud Storage path to write partial failures to. Deprecated. Prefer to use
   * `import_result_gcs_sink`.
   * @param partialFailureGcsSink partialFailureGcsSink or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1ImportRagFilesConfig setPartialFailureGcsSink(GoogleCloudAiplatformV1beta1GcsDestination partialFailureGcsSink) {
    this.partialFailureGcsSink = partialFailureGcsSink;
    return this;
  }

  /**
   * Specifies the size and overlap of chunks after importing RagFiles.
   * @return value or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1RagFileChunkingConfig getRagFileChunkingConfig() {
    return ragFileChunkingConfig;
  }

  /**
   * Specifies the size and overlap of chunks after importing RagFiles.
   * @param ragFileChunkingConfig ragFileChunkingConfig or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1ImportRagFilesConfig setRagFileChunkingConfig(GoogleCloudAiplatformV1beta1RagFileChunkingConfig ragFileChunkingConfig) {
    this.ragFileChunkingConfig = ragFileChunkingConfig;
    return this;
  }

  /**
   * Optional. Specifies the parsing config for RagFiles. RAG will use the default parser if this
   * field is not set.
   * @return value or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1RagFileParsingConfig getRagFileParsingConfig() {
    return ragFileParsingConfig;
  }

  /**
   * Optional. Specifies the parsing config for RagFiles. RAG will use the default parser if this
   * field is not set.
   * @param ragFileParsingConfig ragFileParsingConfig or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1ImportRagFilesConfig setRagFileParsingConfig(GoogleCloudAiplatformV1beta1RagFileParsingConfig ragFileParsingConfig) {
    this.ragFileParsingConfig = ragFileParsingConfig;
    return this;
  }

  /**
   * Specifies the transformation config for RagFiles.
   * @return value or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1RagFileTransformationConfig getRagFileTransformationConfig() {
    return ragFileTransformationConfig;
  }

  /**
   * Specifies the transformation config for RagFiles.
   * @param ragFileTransformationConfig ragFileTransformationConfig or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1ImportRagFilesConfig setRagFileTransformationConfig(GoogleCloudAiplatformV1beta1RagFileTransformationConfig ragFileTransformationConfig) {
    this.ragFileTransformationConfig = ragFileTransformationConfig;
    return this;
  }

  /**
   * SharePoint sources.
   * @return value or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1SharePointSources getSharePointSources() {
    return sharePointSources;
  }

  /**
   * SharePoint sources.
   * @param sharePointSources sharePointSources or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1ImportRagFilesConfig setSharePointSources(GoogleCloudAiplatformV1beta1SharePointSources sharePointSources) {
    this.sharePointSources = sharePointSources;
    return this;
  }

  /**
   * Slack channels with their corresponding access tokens.
   * @return value or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1SlackSource getSlackSource() {
    return slackSource;
  }

  /**
   * Slack channels with their corresponding access tokens.
   * @param slackSource slackSource or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1ImportRagFilesConfig setSlackSource(GoogleCloudAiplatformV1beta1SlackSource slackSource) {
    this.slackSource = slackSource;
    return this;
  }

  @Override
  public GoogleCloudAiplatformV1beta1ImportRagFilesConfig set(String fieldName, Object value) {
    return (GoogleCloudAiplatformV1beta1ImportRagFilesConfig) super.set(fieldName, value);
  }

  @Override
  public GoogleCloudAiplatformV1beta1ImportRagFilesConfig clone() {
    return (GoogleCloudAiplatformV1beta1ImportRagFilesConfig) super.clone();
  }

}
