/*
 * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
 * in compliance with the License. You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License
 * is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
 * or implied. See the License for the specific language governing permissions and limitations under
 * the License.
 */
/*
 * This code was generated by https://github.com/googleapis/google-api-java-client-services/
 * Modify at your own risk.
 */

package com.google.api.services.contentwarehouse.v1.model;

/**
 * Fetcher -> FetchClient FetchReplyData is the metadata for a reply from a FetchRequest. For
 * metadata + document body, FetchReply is further below. NOTE: FetchReplyData (and FetchReply) is
 * the output interface from Multiverse. Teams outside Multiverse/Trawler should not create fake
 * FetchReplies. Trawler: When adding new fields here, it is recommended that at least the following
 * be rebuilt and pushed: - cron_fetcher_index mapreduces: so that UrlReplyIndex, etc. retain the
 * new fields - tlookup, tlookup_server: want to be able to return the new fields - logviewer,
 * fetchutil: annoying to get back 'tag88:' in results -------------------------- Next Tag: 124
 * -----------------------
 *
 * <p> This is the Java data model class that specifies how to parse/serialize into the JSON that is
 * transmitted over HTTP when working with the Document AI Warehouse API. For a detailed explanation
 * see:
 * <a href="https://developers.google.com/api-client-library/java/google-http-java-client/json">https://developers.google.com/api-client-library/java/google-http-java-client/json</a>
 * </p>
 *
 * @author Google, Inc.
 */
@SuppressWarnings("javadoc")
public final class TrawlerFetchReplyData extends com.google.api.client.json.GenericJson {

  /**
   * This field, if non-empty, contains the SSL certificate chain from the server. The filed should
   * be serialized SSLCertificateInfo protobuf, although it used to be text format. Hence, one
   * should ideally use trawler::CertificateUtil to check this field and understand in more detail.
   * This field is populated in two cases: (1) something is wrong with the server certificate and we
   * cannot verify the server's identity. In this case the URL most likely won't display in a
   * browser; (2) if you turned on WantSSLCertificateChain in the FetchRequest. In this case the
   * server certificate may be perfectly fine (despite the field name). This is for the initial hop;
   * additional hops are in Redirects group.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("BadSSLCertificate")
  private java.lang.String badSSLCertificate;

  /**
   * Some client specific data that's set by client in the FetchRequest, and we just copy it here
   * verbatim. This is similar to ClientInfo that we copy from FetchRequest to FetchReply, but this
   * is copied to FetchReplyData, thus stored in trawler logs so can be useful for debugging cases.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("ClientServiceInfo")
  private TrawlerClientServiceInfo clientServiceInfo;

  /**
   * Is the associated body compressed ?
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("CompressedBody")
  private java.lang.Boolean compressedBody;

  /**
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("CrawlTimes")
  private TrawlerCrawlTimes crawlTimes;

  /**
   * Sometimes the hostid and destination IP in the FetchReplyData are not for the hostname in the
   * url. If that's the case DNSHost will be the host that we have used when resolving hostid and
   * DNS. Right now there are two cases: (1) malware team provides a proxy IP:Port to us, so DNSHost
   * will be the proxy IP; and (2) PSS team provides a reference DNS host; so DNSHost will be the
   * reference DNS host.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("DNSHost")
  private java.lang.String dNSHost;

  /**
   * The download time for this fetch (ms). This is the RTT time between fetcher and HOPE, note it
   * does not include time from redirects, just initial hop. If you want the sum of the DownloadTime
   * values for all fetches in the redirect chain, then use the DownLoadTime value in the
   * FetchStats.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("DownloadTime")
  private java.lang.Integer downloadTime;

  /**
   * If present, the edge region that we have used.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("EgressRegion")
  private java.lang.String egressRegion;

  /**
   * If present, it means this host might be eligible for geo crawl. However, this does not mean we
   * enable geo-crawl for this request. Check "GeoCrawlEgressRegion" instead to see if this fetch is
   * conducted via geo crawl.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("EligibleGeoCrawlEgressRegion")
  private java.lang.String eligibleGeoCrawlEgressRegion;

  /**
   * ------- If fetched, the IP from which we fetched, as well as source IP and ports. It is
   * recommended to use trawler::DestinationIP()/HasDestinationIP() accessors, which return a proper
   * IPAddress.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("Endpoints")
  private TrawlerTCPIPInfo endpoints;

  /**
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("Events")
  private java.util.List<TrawlerEvent> events;

  static {
    // hack to force ProGuard to consider TrawlerEvent used, since otherwise it would be stripped out
    // see https://github.com/google/google-api-java-client/issues/543
    com.google.api.client.util.Data.nullOf(TrawlerEvent.class);
  }

  /**
   * With the introduction of fetch pattern based hostload exceptions, one hostid may have multiple
   * hostload buckets, each with its own hostload. In this case, FetchPatternFp will be set to
   * identify the hostload bucket within the hostid. Note this field is only meaningful for the
   * HostBucketData which is recorded only when the client requests to have as part of reply.
   * However, this field is useful for certain stats gathering, so we choose to always record it if
   * its value is available during the fetch.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("FetchPatternFp") @com.google.api.client.json.JsonString
  private java.math.BigInteger fetchPatternFp;

  /**
   * If present, fetch was conducted using floonet and this is the location of floonet egress point
   * we used.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("FlooEgressRegion")
  private java.lang.String flooEgressRegion;

  /**
   * If present, the last hop of the fetch was conducted using floonet and this is the location of
   * floonet egress point. It is different from EgressRegion and FlooEgressREgion because it is a
   * Trawler transparent routing configured in the geo crawl rules(go/da-geo-crawl).
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("GeoCrawlEgressRegion")
  private java.lang.String geoCrawlEgressRegion;

  /**
   * Whether we fallback from geo crawl to local crawl during fetch. The fallback could happen in
   * any hops and there can be at most one fallback because once fallback happens, we will not try
   * geo-crawl anymore.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("GeoCrawlFallback")
  private java.lang.Boolean geoCrawlFallback;

  /**
   * Set only when GeoCrawlFallback is true. Logs the geo crawl location we attempted but failed for
   * this request.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("GeoCrawlLocationAttempted")
  private java.lang.String geoCrawlLocationAttempted;

  /**
   * Set to: o HSTS_STATUS_NONE if there was no HSTS policy match for the URL's host. o
   * HSTS_STATUS_AVAILABLE if there was an HSTS policy, but the URL was not rewritten from HTTP to
   * HTTPS because enable_hsts was not set in client capability config. o HSTS_STATUS_REWRITTEN if
   * the HSTS policy was followed and url was rewritten from HTTP to HTTPS. This field only pertains
   * to the current URL fetch and does not explain a redirect's HSTS status. However,
   * FetchReplyData.Redirects have their own HSTSInfo.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("HSTSInfo")
  private java.lang.String hSTSInfo;

  /**
   * The received HTTP trailers if available.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("HTTPTrailers")
  private java.util.List<TrawlerFetchReplyDataHTTPHeader> hTTPTrailers;

  /**
   * Returns the cache key used when doing cache lookup/update, on a per-hop basis (initial hop)
   * Note this field will not be set if cache lookup/update is disabled/skipped.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("HopCacheKeyForLookup") @com.google.api.client.json.JsonString
  private java.math.BigInteger hopCacheKeyForLookup;

  /**
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("HopCacheKeyForUpdate") @com.google.api.client.json.JsonString
  private java.math.BigInteger hopCacheKeyForUpdate;

  /**
   * Returns trawler::ReuseInfo with status of IMS/IMF/cache query, on a per-hop basis (initial hop)
   * For example, if the URL redirect chain is [URL A] --> [URL B] --> [URL C], this field stores
   * the reuse info of [URL A].
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("HopReuseInfo")
  private java.lang.String hopReuseInfo;

  /**
   * Extra information in robots.txt for this page (integer: or'ed together of type
   * trawler::RobotsInfo) on a per-hop basis (initial hop)
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("HopRobotsInfo")
  private java.lang.Integer hopRobotsInfo;

  /**
   * Data about the host bucket this request is in (if desired) Please talk with Trawler team before
   * considering using this, since what we fill in here is subject to change.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("HostBucketData")
  private TrawlerHostBucketData hostBucketData;

  /**
   * If known, the trawler::HostId that identifies the host (initial hop).
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("HostId") @com.google.api.client.json.JsonString
  private java.math.BigInteger hostId;

  /**
   * The http protocol we send to fetch this URL. This will only be set if the request is using http
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("HttpProtocol")
  private java.lang.String httpProtocol;

  /**
   * The HTTP headers we sent to fetch this URL (initial hop). Not normally filled in, unless
   * FetchParams.WantSentHeaders is set.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("HttpRequestHeaders")
  private java.lang.String httpRequestHeaders;

  /**
   * HTTP headers from the response (initial hop). Trawler does not fill this in; this is intended
   * as a placeholder for crawls like webmirror that fill in and want to track this across redirect
   * hops.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("HttpResponseHeaders")
  private java.lang.String httpResponseHeaders;

  /**
   * Stores the HTTP version we used in the final hop.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("HttpVersion")
  private java.lang.String httpVersion;

  /**
   * Same as the ID of the matching request (used for matching internal fetchclient data in
   * request/reply)
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("ID") @com.google.api.client.json.JsonString
  private java.math.BigInteger iD;

  /**
   * Crawl status of the last url on chain
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("LastUrlStatus")
  private TrawlerFetchStatus lastUrlStatus;

  /**
   * Trawler can optionally add a policy label to a FetchReply. Some uses: - "spam" label via
   * trawler_site_info - "roboted:googlebot" label as a signal to crawls supporting multiple
   * useragents that it's not safe to share the fetch replies with googlebot crawls.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("PolicyData")
  private java.util.List<TrawlerPolicyData> policyData;

  /**
   * If the fetch uses HTTP POST, PUT, or PATCH protocol, and WantPostData is true, the POST data
   * will be copied here. This is only for initial hop. If there are redirects, HTTP POST will be
   * changed to GET on subsequent hops, and the PostData will be cleared. There is only one
   * exception, if the HTTP response code to the POST request is 307 (a new code introduced in
   * RFC7321, sec. 6.4.7), we will preserve the request method and the PostData for the next hop.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("PostData")
  private java.lang.String postData;

  /**
   * This is available only if a fetch results in TIMEOUT_WEB, and we were able to predict, based on
   * content length and bandwidth we were using, how much time (in ms) would be needed to download
   * the entire content.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("PredictedDownloadTimeMs")
  private java.lang.Integer predictedDownloadTimeMs;

  /**
   * Whether we fallback from HTTP/2 to HTTP/1.1 during fetch. The fallback could happen in any hops
   * and there can be at most one fallback because once fallback happens, we will not try HTTP/2
   * anymore.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("ProtocolVersionFallback")
  private java.lang.Boolean protocolVersionFallback;

  /**
   * If this fetch was a result of a redirect, we populate the parent ID here.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("RedirectSourceFetchId") @com.google.api.client.json.JsonString
  private java.math.BigInteger redirectSourceFetchId;

  /**
   * RequestorId is the same on as in the request that triggers this reply -- mainly for diagnostics
   * purpose
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("RequestorID")
  private java.lang.String requestorID;

  /**
   * Machine that sent Trawler this request, for logging. An IPAddress object, packed as a string.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("RequestorIPAddressPacked")
  private java.lang.String requestorIPAddressPacked;

  /**
   * -------- Returns trawler::ReuseInfo with status of IMS/IMF/cache query. Consider using
   * HopReuseInfo instead, which has per-redirect hop detail. If there's URL redirection, this field
   * stores the reuse info of the last hop. For example, if the and URL redirect chain is [URL A]
   * --> [URL B] --> [URL C], this field stores the reuse info of [URL C].
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("ReuseInfo")
  private java.lang.String reuseInfo;

  /**
   * Extra information in robots.txt for this page (ORed together bits from trawler::RobotsInfo).
   * e.g. nosnippet vs. noarchive vs nofollow vs noindex vs disallow Consider using HopRobotsInfo
   * instead, which has per-redirect hop detail.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("RobotsInfo")
  private java.lang.Integer robotsInfo;

  /**
   * Status of the robots.txt fetch. Currently, this is present if: - Certain robots error cases,
   * such as URL_TIMEOUT-TIMEOUT_ROBOTS or URL_UNREACHABLE-UNREACHABLE_ROBOTS_ERROR. - If
   * WantRobotsBody is set in the FetchParams.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("RobotsStatus")
  private TrawlerFetchStatus robotsStatus;

  /**
   * The robots.txt we used for this URL (initial hop). Not normally filled in unless WantRobotsBody
   * is set. This is mostly for debugging purposes and should not be used for large volumes of
   * traffic.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("RobotsTxt")
  private java.lang.String robotsTxt;

  /**
   * Status of the fetch - refers to the final status at the end of the redirect chain.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("Status")
  private TrawlerFetchStatus status;

  /**
   * If present, Client API will enforce the contained constraints
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("ThrottleClient")
  private TrawlerThrottleClientData throttleClient;

  /**
   * Sometimes we throw away content because we cannot store it in the internal buffers. These is
   * how many bytes we have thrown away for this factor.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("ThrownAwayBytes") @com.google.api.client.json.JsonString
  private java.lang.Long thrownAwayBytes;

  /**
   * When this reply came back from fetcher NOTE: TimestampInMS is used for internal debugging. To
   * see when a document was crawled, check CrawlDates.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("TimestampInMS") @com.google.api.client.json.JsonString
  private java.lang.Long timestampInMS;

  /**
   * How many raw bytes we read from the connection to the server when we fetched the page. Includes
   * everything: HTTP headers, overhead for HTTP chunked encoding, whatever compressed/uncompressed
   * form (i.e. gzip/deflate accept-encoding) the content was sent in, etc. This is NOT the same as
   * the size of the uncompressed FetchReply::Body - if the webserver used gzip encoding, this value
   * might be much smaller, since it only counts the compressed wire size. To illustrate, think of 3
   * sizes: 1) TotalFetchedSize - amount Trawler read over the wire from the server. If they used
   * gzip/deflate, this might be 4-5x smaller than the body. 2) UnTruncatedSize/CutoffSize - how big
   * is the full document, after uncompressing any gzip/deflate encoding? If truncated, this is
   * reflected in CutoffSize. 3) FetchReply::Body size - most crawls enable Trawler compression to
   * save storage space (gzip + a google html dictionary). The body size that the end Trawler client
   * sees is post-compression.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("TotalFetchedSize") @com.google.api.client.json.JsonString
  private java.lang.Long totalFetchedSize;

  /**
   * If the url got rewriten by transparent rewrites, here it is the series of rewrites it got
   * through. The fetched one is the last
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("TransparentRewrites")
  private java.util.List<java.lang.String> transparentRewrites;

  /**
   * For logging only; not present in the actual fetcher response
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("TrawlerPrivate")
  private TrawlerTrawlerPrivateFetchReplyData trawlerPrivate;

  /**
   * The original url in the request we are answering. Even though "optional," url must be filled in
   * on all well-formed replies. Trawler guarantees that it is filled in, and basically every client
   * expects it (CHECKs in some cases). -> Not filling this field in is a bug, if you share this
   * data with other crawls/pipelines. You should expect everybody else to require a url.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("Url")
  private java.lang.String url;

  /**
   * Encoding info for the original url itself. Bitfield encoding; see UrlEncoding::{Set,Get}Value
   * in webutil/urlencoding.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("UrlEncoding")
  private java.lang.Integer urlEncoding;

  /**
   * Use the special compression dictionary for uncompressing this. (trawler::kHtmlCompressionDict.
   * Use trawler::FetchReplyUncompressor to uncompress; crawler/trawler/public/fetchreply-util.h)
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key("UseHtmlCompressDictionary")
  private java.lang.Boolean useHtmlCompressDictionary;

  /**
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private TrawlerFetchReplyDataCrawlDates crawldates;

  /**
   * Transfer operation detailed report.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private TrawlerFetchReplyDataDeliveryReport deliveryReport;

  /**
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private TrawlerFetchReplyDataFetchStats fetchstats;

  /**
   * If the input url in FetchRequest is Amazon S3 protocol or Apple Itunes protocol, we will
   * translate it into https url and log it as https url. In the meantime we will store the original
   * s3/itunes url in this field. Before sending back to client, the Url will be translated back to
   * s3 and this field will be cleard.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.String originalProtocolUrl;

  /**
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private TrawlerFetchReplyDataPartialResponse partialresponse;

  /**
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private TrawlerFetchReplyDataProtocolResponse protocolresponse;

  /**
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.util.List<TrawlerFetchReplyDataRedirects> redirects;

  /**
   * Traffic type of this fetch.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.String trafficType;

  /**
   * This field, if non-empty, contains the SSL certificate chain from the server. The filed should
   * be serialized SSLCertificateInfo protobuf, although it used to be text format. Hence, one
   * should ideally use trawler::CertificateUtil to check this field and understand in more detail.
   * This field is populated in two cases: (1) something is wrong with the server certificate and we
   * cannot verify the server's identity. In this case the URL most likely won't display in a
   * browser; (2) if you turned on WantSSLCertificateChain in the FetchRequest. In this case the
   * server certificate may be perfectly fine (despite the field name). This is for the initial hop;
   * additional hops are in Redirects group.
   * @see #decodeBadSSLCertificate()
   * @return value or {@code null} for none
   */
  public java.lang.String getBadSSLCertificate() {
    return badSSLCertificate;
  }

  /**
   * This field, if non-empty, contains the SSL certificate chain from the server. The filed should
   * be serialized SSLCertificateInfo protobuf, although it used to be text format. Hence, one
   * should ideally use trawler::CertificateUtil to check this field and understand in more detail.
   * This field is populated in two cases: (1) something is wrong with the server certificate and we
   * cannot verify the server's identity. In this case the URL most likely won't display in a
   * browser; (2) if you turned on WantSSLCertificateChain in the FetchRequest. In this case the
   * server certificate may be perfectly fine (despite the field name). This is for the initial hop;
   * additional hops are in Redirects group.
   * @see #getBadSSLCertificate()
   * @return Base64 decoded value or {@code null} for none
   *
   * @since 1.14
   */
  public byte[] decodeBadSSLCertificate() {
    return com.google.api.client.util.Base64.decodeBase64(badSSLCertificate);
  }

  /**
   * This field, if non-empty, contains the SSL certificate chain from the server. The filed should
   * be serialized SSLCertificateInfo protobuf, although it used to be text format. Hence, one
   * should ideally use trawler::CertificateUtil to check this field and understand in more detail.
   * This field is populated in two cases: (1) something is wrong with the server certificate and we
   * cannot verify the server's identity. In this case the URL most likely won't display in a
   * browser; (2) if you turned on WantSSLCertificateChain in the FetchRequest. In this case the
   * server certificate may be perfectly fine (despite the field name). This is for the initial hop;
   * additional hops are in Redirects group.
   * @see #encodeBadSSLCertificate()
   * @param badSSLCertificate badSSLCertificate or {@code null} for none
   */
  public TrawlerFetchReplyData setBadSSLCertificate(java.lang.String badSSLCertificate) {
    this.badSSLCertificate = badSSLCertificate;
    return this;
  }

  /**
   * This field, if non-empty, contains the SSL certificate chain from the server. The filed should
   * be serialized SSLCertificateInfo protobuf, although it used to be text format. Hence, one
   * should ideally use trawler::CertificateUtil to check this field and understand in more detail.
   * This field is populated in two cases: (1) something is wrong with the server certificate and we
   * cannot verify the server's identity. In this case the URL most likely won't display in a
   * browser; (2) if you turned on WantSSLCertificateChain in the FetchRequest. In this case the
   * server certificate may be perfectly fine (despite the field name). This is for the initial hop;
   * additional hops are in Redirects group.
   * @see #setBadSSLCertificate()
   *
   * <p>
   * The value is encoded Base64 or {@code null} for none.
   * </p>
   *
   * @since 1.14
   */
  public TrawlerFetchReplyData encodeBadSSLCertificate(byte[] badSSLCertificate) {
    this.badSSLCertificate = com.google.api.client.util.Base64.encodeBase64URLSafeString(badSSLCertificate);
    return this;
  }

  /**
   * Some client specific data that's set by client in the FetchRequest, and we just copy it here
   * verbatim. This is similar to ClientInfo that we copy from FetchRequest to FetchReply, but this
   * is copied to FetchReplyData, thus stored in trawler logs so can be useful for debugging cases.
   * @return value or {@code null} for none
   */
  public TrawlerClientServiceInfo getClientServiceInfo() {
    return clientServiceInfo;
  }

  /**
   * Some client specific data that's set by client in the FetchRequest, and we just copy it here
   * verbatim. This is similar to ClientInfo that we copy from FetchRequest to FetchReply, but this
   * is copied to FetchReplyData, thus stored in trawler logs so can be useful for debugging cases.
   * @param clientServiceInfo clientServiceInfo or {@code null} for none
   */
  public TrawlerFetchReplyData setClientServiceInfo(TrawlerClientServiceInfo clientServiceInfo) {
    this.clientServiceInfo = clientServiceInfo;
    return this;
  }

  /**
   * Is the associated body compressed ?
   * @return value or {@code null} for none
   */
  public java.lang.Boolean getCompressedBody() {
    return compressedBody;
  }

  /**
   * Is the associated body compressed ?
   * @param compressedBody compressedBody or {@code null} for none
   */
  public TrawlerFetchReplyData setCompressedBody(java.lang.Boolean compressedBody) {
    this.compressedBody = compressedBody;
    return this;
  }

  /**
   * @return value or {@code null} for none
   */
  public TrawlerCrawlTimes getCrawlTimes() {
    return crawlTimes;
  }

  /**
   * @param crawlTimes crawlTimes or {@code null} for none
   */
  public TrawlerFetchReplyData setCrawlTimes(TrawlerCrawlTimes crawlTimes) {
    this.crawlTimes = crawlTimes;
    return this;
  }

  /**
   * Sometimes the hostid and destination IP in the FetchReplyData are not for the hostname in the
   * url. If that's the case DNSHost will be the host that we have used when resolving hostid and
   * DNS. Right now there are two cases: (1) malware team provides a proxy IP:Port to us, so DNSHost
   * will be the proxy IP; and (2) PSS team provides a reference DNS host; so DNSHost will be the
   * reference DNS host.
   * @return value or {@code null} for none
   */
  public java.lang.String getDNSHost() {
    return dNSHost;
  }

  /**
   * Sometimes the hostid and destination IP in the FetchReplyData are not for the hostname in the
   * url. If that's the case DNSHost will be the host that we have used when resolving hostid and
   * DNS. Right now there are two cases: (1) malware team provides a proxy IP:Port to us, so DNSHost
   * will be the proxy IP; and (2) PSS team provides a reference DNS host; so DNSHost will be the
   * reference DNS host.
   * @param dNSHost dNSHost or {@code null} for none
   */
  public TrawlerFetchReplyData setDNSHost(java.lang.String dNSHost) {
    this.dNSHost = dNSHost;
    return this;
  }

  /**
   * The download time for this fetch (ms). This is the RTT time between fetcher and HOPE, note it
   * does not include time from redirects, just initial hop. If you want the sum of the DownloadTime
   * values for all fetches in the redirect chain, then use the DownLoadTime value in the
   * FetchStats.
   * @return value or {@code null} for none
   */
  public java.lang.Integer getDownloadTime() {
    return downloadTime;
  }

  /**
   * The download time for this fetch (ms). This is the RTT time between fetcher and HOPE, note it
   * does not include time from redirects, just initial hop. If you want the sum of the DownloadTime
   * values for all fetches in the redirect chain, then use the DownLoadTime value in the
   * FetchStats.
   * @param downloadTime downloadTime or {@code null} for none
   */
  public TrawlerFetchReplyData setDownloadTime(java.lang.Integer downloadTime) {
    this.downloadTime = downloadTime;
    return this;
  }

  /**
   * If present, the edge region that we have used.
   * @return value or {@code null} for none
   */
  public java.lang.String getEgressRegion() {
    return egressRegion;
  }

  /**
   * If present, the edge region that we have used.
   * @param egressRegion egressRegion or {@code null} for none
   */
  public TrawlerFetchReplyData setEgressRegion(java.lang.String egressRegion) {
    this.egressRegion = egressRegion;
    return this;
  }

  /**
   * If present, it means this host might be eligible for geo crawl. However, this does not mean we
   * enable geo-crawl for this request. Check "GeoCrawlEgressRegion" instead to see if this fetch is
   * conducted via geo crawl.
   * @return value or {@code null} for none
   */
  public java.lang.String getEligibleGeoCrawlEgressRegion() {
    return eligibleGeoCrawlEgressRegion;
  }

  /**
   * If present, it means this host might be eligible for geo crawl. However, this does not mean we
   * enable geo-crawl for this request. Check "GeoCrawlEgressRegion" instead to see if this fetch is
   * conducted via geo crawl.
   * @param eligibleGeoCrawlEgressRegion eligibleGeoCrawlEgressRegion or {@code null} for none
   */
  public TrawlerFetchReplyData setEligibleGeoCrawlEgressRegion(java.lang.String eligibleGeoCrawlEgressRegion) {
    this.eligibleGeoCrawlEgressRegion = eligibleGeoCrawlEgressRegion;
    return this;
  }

  /**
   * ------- If fetched, the IP from which we fetched, as well as source IP and ports. It is
   * recommended to use trawler::DestinationIP()/HasDestinationIP() accessors, which return a proper
   * IPAddress.
   * @return value or {@code null} for none
   */
  public TrawlerTCPIPInfo getEndpoints() {
    return endpoints;
  }

  /**
   * ------- If fetched, the IP from which we fetched, as well as source IP and ports. It is
   * recommended to use trawler::DestinationIP()/HasDestinationIP() accessors, which return a proper
   * IPAddress.
   * @param endpoints endpoints or {@code null} for none
   */
  public TrawlerFetchReplyData setEndpoints(TrawlerTCPIPInfo endpoints) {
    this.endpoints = endpoints;
    return this;
  }

  /**
   * @return value or {@code null} for none
   */
  public java.util.List<TrawlerEvent> getEvents() {
    return events;
  }

  /**
   * @param events events or {@code null} for none
   */
  public TrawlerFetchReplyData setEvents(java.util.List<TrawlerEvent> events) {
    this.events = events;
    return this;
  }

  /**
   * With the introduction of fetch pattern based hostload exceptions, one hostid may have multiple
   * hostload buckets, each with its own hostload. In this case, FetchPatternFp will be set to
   * identify the hostload bucket within the hostid. Note this field is only meaningful for the
   * HostBucketData which is recorded only when the client requests to have as part of reply.
   * However, this field is useful for certain stats gathering, so we choose to always record it if
   * its value is available during the fetch.
   * @return value or {@code null} for none
   */
  public java.math.BigInteger getFetchPatternFp() {
    return fetchPatternFp;
  }

  /**
   * With the introduction of fetch pattern based hostload exceptions, one hostid may have multiple
   * hostload buckets, each with its own hostload. In this case, FetchPatternFp will be set to
   * identify the hostload bucket within the hostid. Note this field is only meaningful for the
   * HostBucketData which is recorded only when the client requests to have as part of reply.
   * However, this field is useful for certain stats gathering, so we choose to always record it if
   * its value is available during the fetch.
   * @param fetchPatternFp fetchPatternFp or {@code null} for none
   */
  public TrawlerFetchReplyData setFetchPatternFp(java.math.BigInteger fetchPatternFp) {
    this.fetchPatternFp = fetchPatternFp;
    return this;
  }

  /**
   * If present, fetch was conducted using floonet and this is the location of floonet egress point
   * we used.
   * @return value or {@code null} for none
   */
  public java.lang.String getFlooEgressRegion() {
    return flooEgressRegion;
  }

  /**
   * If present, fetch was conducted using floonet and this is the location of floonet egress point
   * we used.
   * @param flooEgressRegion flooEgressRegion or {@code null} for none
   */
  public TrawlerFetchReplyData setFlooEgressRegion(java.lang.String flooEgressRegion) {
    this.flooEgressRegion = flooEgressRegion;
    return this;
  }

  /**
   * If present, the last hop of the fetch was conducted using floonet and this is the location of
   * floonet egress point. It is different from EgressRegion and FlooEgressREgion because it is a
   * Trawler transparent routing configured in the geo crawl rules(go/da-geo-crawl).
   * @return value or {@code null} for none
   */
  public java.lang.String getGeoCrawlEgressRegion() {
    return geoCrawlEgressRegion;
  }

  /**
   * If present, the last hop of the fetch was conducted using floonet and this is the location of
   * floonet egress point. It is different from EgressRegion and FlooEgressREgion because it is a
   * Trawler transparent routing configured in the geo crawl rules(go/da-geo-crawl).
   * @param geoCrawlEgressRegion geoCrawlEgressRegion or {@code null} for none
   */
  public TrawlerFetchReplyData setGeoCrawlEgressRegion(java.lang.String geoCrawlEgressRegion) {
    this.geoCrawlEgressRegion = geoCrawlEgressRegion;
    return this;
  }

  /**
   * Whether we fallback from geo crawl to local crawl during fetch. The fallback could happen in
   * any hops and there can be at most one fallback because once fallback happens, we will not try
   * geo-crawl anymore.
   * @return value or {@code null} for none
   */
  public java.lang.Boolean getGeoCrawlFallback() {
    return geoCrawlFallback;
  }

  /**
   * Whether we fallback from geo crawl to local crawl during fetch. The fallback could happen in
   * any hops and there can be at most one fallback because once fallback happens, we will not try
   * geo-crawl anymore.
   * @param geoCrawlFallback geoCrawlFallback or {@code null} for none
   */
  public TrawlerFetchReplyData setGeoCrawlFallback(java.lang.Boolean geoCrawlFallback) {
    this.geoCrawlFallback = geoCrawlFallback;
    return this;
  }

  /**
   * Set only when GeoCrawlFallback is true. Logs the geo crawl location we attempted but failed for
   * this request.
   * @return value or {@code null} for none
   */
  public java.lang.String getGeoCrawlLocationAttempted() {
    return geoCrawlLocationAttempted;
  }

  /**
   * Set only when GeoCrawlFallback is true. Logs the geo crawl location we attempted but failed for
   * this request.
   * @param geoCrawlLocationAttempted geoCrawlLocationAttempted or {@code null} for none
   */
  public TrawlerFetchReplyData setGeoCrawlLocationAttempted(java.lang.String geoCrawlLocationAttempted) {
    this.geoCrawlLocationAttempted = geoCrawlLocationAttempted;
    return this;
  }

  /**
   * Set to: o HSTS_STATUS_NONE if there was no HSTS policy match for the URL's host. o
   * HSTS_STATUS_AVAILABLE if there was an HSTS policy, but the URL was not rewritten from HTTP to
   * HTTPS because enable_hsts was not set in client capability config. o HSTS_STATUS_REWRITTEN if
   * the HSTS policy was followed and url was rewritten from HTTP to HTTPS. This field only pertains
   * to the current URL fetch and does not explain a redirect's HSTS status. However,
   * FetchReplyData.Redirects have their own HSTSInfo.
   * @return value or {@code null} for none
   */
  public java.lang.String getHSTSInfo() {
    return hSTSInfo;
  }

  /**
   * Set to: o HSTS_STATUS_NONE if there was no HSTS policy match for the URL's host. o
   * HSTS_STATUS_AVAILABLE if there was an HSTS policy, but the URL was not rewritten from HTTP to
   * HTTPS because enable_hsts was not set in client capability config. o HSTS_STATUS_REWRITTEN if
   * the HSTS policy was followed and url was rewritten from HTTP to HTTPS. This field only pertains
   * to the current URL fetch and does not explain a redirect's HSTS status. However,
   * FetchReplyData.Redirects have their own HSTSInfo.
   * @param hSTSInfo hSTSInfo or {@code null} for none
   */
  public TrawlerFetchReplyData setHSTSInfo(java.lang.String hSTSInfo) {
    this.hSTSInfo = hSTSInfo;
    return this;
  }

  /**
   * The received HTTP trailers if available.
   * @return value or {@code null} for none
   */
  public java.util.List<TrawlerFetchReplyDataHTTPHeader> getHTTPTrailers() {
    return hTTPTrailers;
  }

  /**
   * The received HTTP trailers if available.
   * @param hTTPTrailers hTTPTrailers or {@code null} for none
   */
  public TrawlerFetchReplyData setHTTPTrailers(java.util.List<TrawlerFetchReplyDataHTTPHeader> hTTPTrailers) {
    this.hTTPTrailers = hTTPTrailers;
    return this;
  }

  /**
   * Returns the cache key used when doing cache lookup/update, on a per-hop basis (initial hop)
   * Note this field will not be set if cache lookup/update is disabled/skipped.
   * @return value or {@code null} for none
   */
  public java.math.BigInteger getHopCacheKeyForLookup() {
    return hopCacheKeyForLookup;
  }

  /**
   * Returns the cache key used when doing cache lookup/update, on a per-hop basis (initial hop)
   * Note this field will not be set if cache lookup/update is disabled/skipped.
   * @param hopCacheKeyForLookup hopCacheKeyForLookup or {@code null} for none
   */
  public TrawlerFetchReplyData setHopCacheKeyForLookup(java.math.BigInteger hopCacheKeyForLookup) {
    this.hopCacheKeyForLookup = hopCacheKeyForLookup;
    return this;
  }

  /**
   * @return value or {@code null} for none
   */
  public java.math.BigInteger getHopCacheKeyForUpdate() {
    return hopCacheKeyForUpdate;
  }

  /**
   * @param hopCacheKeyForUpdate hopCacheKeyForUpdate or {@code null} for none
   */
  public TrawlerFetchReplyData setHopCacheKeyForUpdate(java.math.BigInteger hopCacheKeyForUpdate) {
    this.hopCacheKeyForUpdate = hopCacheKeyForUpdate;
    return this;
  }

  /**
   * Returns trawler::ReuseInfo with status of IMS/IMF/cache query, on a per-hop basis (initial hop)
   * For example, if the URL redirect chain is [URL A] --> [URL B] --> [URL C], this field stores
   * the reuse info of [URL A].
   * @return value or {@code null} for none
   */
  public java.lang.String getHopReuseInfo() {
    return hopReuseInfo;
  }

  /**
   * Returns trawler::ReuseInfo with status of IMS/IMF/cache query, on a per-hop basis (initial hop)
   * For example, if the URL redirect chain is [URL A] --> [URL B] --> [URL C], this field stores
   * the reuse info of [URL A].
   * @param hopReuseInfo hopReuseInfo or {@code null} for none
   */
  public TrawlerFetchReplyData setHopReuseInfo(java.lang.String hopReuseInfo) {
    this.hopReuseInfo = hopReuseInfo;
    return this;
  }

  /**
   * Extra information in robots.txt for this page (integer: or'ed together of type
   * trawler::RobotsInfo) on a per-hop basis (initial hop)
   * @return value or {@code null} for none
   */
  public java.lang.Integer getHopRobotsInfo() {
    return hopRobotsInfo;
  }

  /**
   * Extra information in robots.txt for this page (integer: or'ed together of type
   * trawler::RobotsInfo) on a per-hop basis (initial hop)
   * @param hopRobotsInfo hopRobotsInfo or {@code null} for none
   */
  public TrawlerFetchReplyData setHopRobotsInfo(java.lang.Integer hopRobotsInfo) {
    this.hopRobotsInfo = hopRobotsInfo;
    return this;
  }

  /**
   * Data about the host bucket this request is in (if desired) Please talk with Trawler team before
   * considering using this, since what we fill in here is subject to change.
   * @return value or {@code null} for none
   */
  public TrawlerHostBucketData getHostBucketData() {
    return hostBucketData;
  }

  /**
   * Data about the host bucket this request is in (if desired) Please talk with Trawler team before
   * considering using this, since what we fill in here is subject to change.
   * @param hostBucketData hostBucketData or {@code null} for none
   */
  public TrawlerFetchReplyData setHostBucketData(TrawlerHostBucketData hostBucketData) {
    this.hostBucketData = hostBucketData;
    return this;
  }

  /**
   * If known, the trawler::HostId that identifies the host (initial hop).
   * @return value or {@code null} for none
   */
  public java.math.BigInteger getHostId() {
    return hostId;
  }

  /**
   * If known, the trawler::HostId that identifies the host (initial hop).
   * @param hostId hostId or {@code null} for none
   */
  public TrawlerFetchReplyData setHostId(java.math.BigInteger hostId) {
    this.hostId = hostId;
    return this;
  }

  /**
   * The http protocol we send to fetch this URL. This will only be set if the request is using http
   * @return value or {@code null} for none
   */
  public java.lang.String getHttpProtocol() {
    return httpProtocol;
  }

  /**
   * The http protocol we send to fetch this URL. This will only be set if the request is using http
   * @param httpProtocol httpProtocol or {@code null} for none
   */
  public TrawlerFetchReplyData setHttpProtocol(java.lang.String httpProtocol) {
    this.httpProtocol = httpProtocol;
    return this;
  }

  /**
   * The HTTP headers we sent to fetch this URL (initial hop). Not normally filled in, unless
   * FetchParams.WantSentHeaders is set.
   * @return value or {@code null} for none
   */
  public java.lang.String getHttpRequestHeaders() {
    return httpRequestHeaders;
  }

  /**
   * The HTTP headers we sent to fetch this URL (initial hop). Not normally filled in, unless
   * FetchParams.WantSentHeaders is set.
   * @param httpRequestHeaders httpRequestHeaders or {@code null} for none
   */
  public TrawlerFetchReplyData setHttpRequestHeaders(java.lang.String httpRequestHeaders) {
    this.httpRequestHeaders = httpRequestHeaders;
    return this;
  }

  /**
   * HTTP headers from the response (initial hop). Trawler does not fill this in; this is intended
   * as a placeholder for crawls like webmirror that fill in and want to track this across redirect
   * hops.
   * @return value or {@code null} for none
   */
  public java.lang.String getHttpResponseHeaders() {
    return httpResponseHeaders;
  }

  /**
   * HTTP headers from the response (initial hop). Trawler does not fill this in; this is intended
   * as a placeholder for crawls like webmirror that fill in and want to track this across redirect
   * hops.
   * @param httpResponseHeaders httpResponseHeaders or {@code null} for none
   */
  public TrawlerFetchReplyData setHttpResponseHeaders(java.lang.String httpResponseHeaders) {
    this.httpResponseHeaders = httpResponseHeaders;
    return this;
  }

  /**
   * Stores the HTTP version we used in the final hop.
   * @return value or {@code null} for none
   */
  public java.lang.String getHttpVersion() {
    return httpVersion;
  }

  /**
   * Stores the HTTP version we used in the final hop.
   * @param httpVersion httpVersion or {@code null} for none
   */
  public TrawlerFetchReplyData setHttpVersion(java.lang.String httpVersion) {
    this.httpVersion = httpVersion;
    return this;
  }

  /**
   * Same as the ID of the matching request (used for matching internal fetchclient data in
   * request/reply)
   * @return value or {@code null} for none
   */
  public java.math.BigInteger getID() {
    return iD;
  }

  /**
   * Same as the ID of the matching request (used for matching internal fetchclient data in
   * request/reply)
   * @param iD iD or {@code null} for none
   */
  public TrawlerFetchReplyData setID(java.math.BigInteger iD) {
    this.iD = iD;
    return this;
  }

  /**
   * Crawl status of the last url on chain
   * @return value or {@code null} for none
   */
  public TrawlerFetchStatus getLastUrlStatus() {
    return lastUrlStatus;
  }

  /**
   * Crawl status of the last url on chain
   * @param lastUrlStatus lastUrlStatus or {@code null} for none
   */
  public TrawlerFetchReplyData setLastUrlStatus(TrawlerFetchStatus lastUrlStatus) {
    this.lastUrlStatus = lastUrlStatus;
    return this;
  }

  /**
   * Trawler can optionally add a policy label to a FetchReply. Some uses: - "spam" label via
   * trawler_site_info - "roboted:googlebot" label as a signal to crawls supporting multiple
   * useragents that it's not safe to share the fetch replies with googlebot crawls.
   * @return value or {@code null} for none
   */
  public java.util.List<TrawlerPolicyData> getPolicyData() {
    return policyData;
  }

  /**
   * Trawler can optionally add a policy label to a FetchReply. Some uses: - "spam" label via
   * trawler_site_info - "roboted:googlebot" label as a signal to crawls supporting multiple
   * useragents that it's not safe to share the fetch replies with googlebot crawls.
   * @param policyData policyData or {@code null} for none
   */
  public TrawlerFetchReplyData setPolicyData(java.util.List<TrawlerPolicyData> policyData) {
    this.policyData = policyData;
    return this;
  }

  /**
   * If the fetch uses HTTP POST, PUT, or PATCH protocol, and WantPostData is true, the POST data
   * will be copied here. This is only for initial hop. If there are redirects, HTTP POST will be
   * changed to GET on subsequent hops, and the PostData will be cleared. There is only one
   * exception, if the HTTP response code to the POST request is 307 (a new code introduced in
   * RFC7321, sec. 6.4.7), we will preserve the request method and the PostData for the next hop.
   * @see #decodePostData()
   * @return value or {@code null} for none
   */
  public java.lang.String getPostData() {
    return postData;
  }

  /**
   * If the fetch uses HTTP POST, PUT, or PATCH protocol, and WantPostData is true, the POST data
   * will be copied here. This is only for initial hop. If there are redirects, HTTP POST will be
   * changed to GET on subsequent hops, and the PostData will be cleared. There is only one
   * exception, if the HTTP response code to the POST request is 307 (a new code introduced in
   * RFC7321, sec. 6.4.7), we will preserve the request method and the PostData for the next hop.
   * @see #getPostData()
   * @return Base64 decoded value or {@code null} for none
   *
   * @since 1.14
   */
  public byte[] decodePostData() {
    return com.google.api.client.util.Base64.decodeBase64(postData);
  }

  /**
   * If the fetch uses HTTP POST, PUT, or PATCH protocol, and WantPostData is true, the POST data
   * will be copied here. This is only for initial hop. If there are redirects, HTTP POST will be
   * changed to GET on subsequent hops, and the PostData will be cleared. There is only one
   * exception, if the HTTP response code to the POST request is 307 (a new code introduced in
   * RFC7321, sec. 6.4.7), we will preserve the request method and the PostData for the next hop.
   * @see #encodePostData()
   * @param postData postData or {@code null} for none
   */
  public TrawlerFetchReplyData setPostData(java.lang.String postData) {
    this.postData = postData;
    return this;
  }

  /**
   * If the fetch uses HTTP POST, PUT, or PATCH protocol, and WantPostData is true, the POST data
   * will be copied here. This is only for initial hop. If there are redirects, HTTP POST will be
   * changed to GET on subsequent hops, and the PostData will be cleared. There is only one
   * exception, if the HTTP response code to the POST request is 307 (a new code introduced in
   * RFC7321, sec. 6.4.7), we will preserve the request method and the PostData for the next hop.
   * @see #setPostData()
   *
   * <p>
   * The value is encoded Base64 or {@code null} for none.
   * </p>
   *
   * @since 1.14
   */
  public TrawlerFetchReplyData encodePostData(byte[] postData) {
    this.postData = com.google.api.client.util.Base64.encodeBase64URLSafeString(postData);
    return this;
  }

  /**
   * This is available only if a fetch results in TIMEOUT_WEB, and we were able to predict, based on
   * content length and bandwidth we were using, how much time (in ms) would be needed to download
   * the entire content.
   * @return value or {@code null} for none
   */
  public java.lang.Integer getPredictedDownloadTimeMs() {
    return predictedDownloadTimeMs;
  }

  /**
   * This is available only if a fetch results in TIMEOUT_WEB, and we were able to predict, based on
   * content length and bandwidth we were using, how much time (in ms) would be needed to download
   * the entire content.
   * @param predictedDownloadTimeMs predictedDownloadTimeMs or {@code null} for none
   */
  public TrawlerFetchReplyData setPredictedDownloadTimeMs(java.lang.Integer predictedDownloadTimeMs) {
    this.predictedDownloadTimeMs = predictedDownloadTimeMs;
    return this;
  }

  /**
   * Whether we fallback from HTTP/2 to HTTP/1.1 during fetch. The fallback could happen in any hops
   * and there can be at most one fallback because once fallback happens, we will not try HTTP/2
   * anymore.
   * @return value or {@code null} for none
   */
  public java.lang.Boolean getProtocolVersionFallback() {
    return protocolVersionFallback;
  }

  /**
   * Whether we fallback from HTTP/2 to HTTP/1.1 during fetch. The fallback could happen in any hops
   * and there can be at most one fallback because once fallback happens, we will not try HTTP/2
   * anymore.
   * @param protocolVersionFallback protocolVersionFallback or {@code null} for none
   */
  public TrawlerFetchReplyData setProtocolVersionFallback(java.lang.Boolean protocolVersionFallback) {
    this.protocolVersionFallback = protocolVersionFallback;
    return this;
  }

  /**
   * If this fetch was a result of a redirect, we populate the parent ID here.
   * @return value or {@code null} for none
   */
  public java.math.BigInteger getRedirectSourceFetchId() {
    return redirectSourceFetchId;
  }

  /**
   * If this fetch was a result of a redirect, we populate the parent ID here.
   * @param redirectSourceFetchId redirectSourceFetchId or {@code null} for none
   */
  public TrawlerFetchReplyData setRedirectSourceFetchId(java.math.BigInteger redirectSourceFetchId) {
    this.redirectSourceFetchId = redirectSourceFetchId;
    return this;
  }

  /**
   * RequestorId is the same on as in the request that triggers this reply -- mainly for diagnostics
   * purpose
   * @return value or {@code null} for none
   */
  public java.lang.String getRequestorID() {
    return requestorID;
  }

  /**
   * RequestorId is the same on as in the request that triggers this reply -- mainly for diagnostics
   * purpose
   * @param requestorID requestorID or {@code null} for none
   */
  public TrawlerFetchReplyData setRequestorID(java.lang.String requestorID) {
    this.requestorID = requestorID;
    return this;
  }

  /**
   * Machine that sent Trawler this request, for logging. An IPAddress object, packed as a string.
   * @see #decodeRequestorIPAddressPacked()
   * @return value or {@code null} for none
   */
  public java.lang.String getRequestorIPAddressPacked() {
    return requestorIPAddressPacked;
  }

  /**
   * Machine that sent Trawler this request, for logging. An IPAddress object, packed as a string.
   * @see #getRequestorIPAddressPacked()
   * @return Base64 decoded value or {@code null} for none
   *
   * @since 1.14
   */
  public byte[] decodeRequestorIPAddressPacked() {
    return com.google.api.client.util.Base64.decodeBase64(requestorIPAddressPacked);
  }

  /**
   * Machine that sent Trawler this request, for logging. An IPAddress object, packed as a string.
   * @see #encodeRequestorIPAddressPacked()
   * @param requestorIPAddressPacked requestorIPAddressPacked or {@code null} for none
   */
  public TrawlerFetchReplyData setRequestorIPAddressPacked(java.lang.String requestorIPAddressPacked) {
    this.requestorIPAddressPacked = requestorIPAddressPacked;
    return this;
  }

  /**
   * Machine that sent Trawler this request, for logging. An IPAddress object, packed as a string.
   * @see #setRequestorIPAddressPacked()
   *
   * <p>
   * The value is encoded Base64 or {@code null} for none.
   * </p>
   *
   * @since 1.14
   */
  public TrawlerFetchReplyData encodeRequestorIPAddressPacked(byte[] requestorIPAddressPacked) {
    this.requestorIPAddressPacked = com.google.api.client.util.Base64.encodeBase64URLSafeString(requestorIPAddressPacked);
    return this;
  }

  /**
   * -------- Returns trawler::ReuseInfo with status of IMS/IMF/cache query. Consider using
   * HopReuseInfo instead, which has per-redirect hop detail. If there's URL redirection, this field
   * stores the reuse info of the last hop. For example, if the and URL redirect chain is [URL A]
   * --> [URL B] --> [URL C], this field stores the reuse info of [URL C].
   * @return value or {@code null} for none
   */
  public java.lang.String getReuseInfo() {
    return reuseInfo;
  }

  /**
   * -------- Returns trawler::ReuseInfo with status of IMS/IMF/cache query. Consider using
   * HopReuseInfo instead, which has per-redirect hop detail. If there's URL redirection, this field
   * stores the reuse info of the last hop. For example, if the and URL redirect chain is [URL A]
   * --> [URL B] --> [URL C], this field stores the reuse info of [URL C].
   * @param reuseInfo reuseInfo or {@code null} for none
   */
  public TrawlerFetchReplyData setReuseInfo(java.lang.String reuseInfo) {
    this.reuseInfo = reuseInfo;
    return this;
  }

  /**
   * Extra information in robots.txt for this page (ORed together bits from trawler::RobotsInfo).
   * e.g. nosnippet vs. noarchive vs nofollow vs noindex vs disallow Consider using HopRobotsInfo
   * instead, which has per-redirect hop detail.
   * @return value or {@code null} for none
   */
  public java.lang.Integer getRobotsInfo() {
    return robotsInfo;
  }

  /**
   * Extra information in robots.txt for this page (ORed together bits from trawler::RobotsInfo).
   * e.g. nosnippet vs. noarchive vs nofollow vs noindex vs disallow Consider using HopRobotsInfo
   * instead, which has per-redirect hop detail.
   * @param robotsInfo robotsInfo or {@code null} for none
   */
  public TrawlerFetchReplyData setRobotsInfo(java.lang.Integer robotsInfo) {
    this.robotsInfo = robotsInfo;
    return this;
  }

  /**
   * Status of the robots.txt fetch. Currently, this is present if: - Certain robots error cases,
   * such as URL_TIMEOUT-TIMEOUT_ROBOTS or URL_UNREACHABLE-UNREACHABLE_ROBOTS_ERROR. - If
   * WantRobotsBody is set in the FetchParams.
   * @return value or {@code null} for none
   */
  public TrawlerFetchStatus getRobotsStatus() {
    return robotsStatus;
  }

  /**
   * Status of the robots.txt fetch. Currently, this is present if: - Certain robots error cases,
   * such as URL_TIMEOUT-TIMEOUT_ROBOTS or URL_UNREACHABLE-UNREACHABLE_ROBOTS_ERROR. - If
   * WantRobotsBody is set in the FetchParams.
   * @param robotsStatus robotsStatus or {@code null} for none
   */
  public TrawlerFetchReplyData setRobotsStatus(TrawlerFetchStatus robotsStatus) {
    this.robotsStatus = robotsStatus;
    return this;
  }

  /**
   * The robots.txt we used for this URL (initial hop). Not normally filled in unless WantRobotsBody
   * is set. This is mostly for debugging purposes and should not be used for large volumes of
   * traffic.
   * @see #decodeRobotsTxt()
   * @return value or {@code null} for none
   */
  public java.lang.String getRobotsTxt() {
    return robotsTxt;
  }

  /**
   * The robots.txt we used for this URL (initial hop). Not normally filled in unless WantRobotsBody
   * is set. This is mostly for debugging purposes and should not be used for large volumes of
   * traffic.
   * @see #getRobotsTxt()
   * @return Base64 decoded value or {@code null} for none
   *
   * @since 1.14
   */
  public byte[] decodeRobotsTxt() {
    return com.google.api.client.util.Base64.decodeBase64(robotsTxt);
  }

  /**
   * The robots.txt we used for this URL (initial hop). Not normally filled in unless WantRobotsBody
   * is set. This is mostly for debugging purposes and should not be used for large volumes of
   * traffic.
   * @see #encodeRobotsTxt()
   * @param robotsTxt robotsTxt or {@code null} for none
   */
  public TrawlerFetchReplyData setRobotsTxt(java.lang.String robotsTxt) {
    this.robotsTxt = robotsTxt;
    return this;
  }

  /**
   * The robots.txt we used for this URL (initial hop). Not normally filled in unless WantRobotsBody
   * is set. This is mostly for debugging purposes and should not be used for large volumes of
   * traffic.
   * @see #setRobotsTxt()
   *
   * <p>
   * The value is encoded Base64 or {@code null} for none.
   * </p>
   *
   * @since 1.14
   */
  public TrawlerFetchReplyData encodeRobotsTxt(byte[] robotsTxt) {
    this.robotsTxt = com.google.api.client.util.Base64.encodeBase64URLSafeString(robotsTxt);
    return this;
  }

  /**
   * Status of the fetch - refers to the final status at the end of the redirect chain.
   * @return value or {@code null} for none
   */
  public TrawlerFetchStatus getStatus() {
    return status;
  }

  /**
   * Status of the fetch - refers to the final status at the end of the redirect chain.
   * @param status status or {@code null} for none
   */
  public TrawlerFetchReplyData setStatus(TrawlerFetchStatus status) {
    this.status = status;
    return this;
  }

  /**
   * If present, Client API will enforce the contained constraints
   * @return value or {@code null} for none
   */
  public TrawlerThrottleClientData getThrottleClient() {
    return throttleClient;
  }

  /**
   * If present, Client API will enforce the contained constraints
   * @param throttleClient throttleClient or {@code null} for none
   */
  public TrawlerFetchReplyData setThrottleClient(TrawlerThrottleClientData throttleClient) {
    this.throttleClient = throttleClient;
    return this;
  }

  /**
   * Sometimes we throw away content because we cannot store it in the internal buffers. These is
   * how many bytes we have thrown away for this factor.
   * @return value or {@code null} for none
   */
  public java.lang.Long getThrownAwayBytes() {
    return thrownAwayBytes;
  }

  /**
   * Sometimes we throw away content because we cannot store it in the internal buffers. These is
   * how many bytes we have thrown away for this factor.
   * @param thrownAwayBytes thrownAwayBytes or {@code null} for none
   */
  public TrawlerFetchReplyData setThrownAwayBytes(java.lang.Long thrownAwayBytes) {
    this.thrownAwayBytes = thrownAwayBytes;
    return this;
  }

  /**
   * When this reply came back from fetcher NOTE: TimestampInMS is used for internal debugging. To
   * see when a document was crawled, check CrawlDates.
   * @return value or {@code null} for none
   */
  public java.lang.Long getTimestampInMS() {
    return timestampInMS;
  }

  /**
   * When this reply came back from fetcher NOTE: TimestampInMS is used for internal debugging. To
   * see when a document was crawled, check CrawlDates.
   * @param timestampInMS timestampInMS or {@code null} for none
   */
  public TrawlerFetchReplyData setTimestampInMS(java.lang.Long timestampInMS) {
    this.timestampInMS = timestampInMS;
    return this;
  }

  /**
   * How many raw bytes we read from the connection to the server when we fetched the page. Includes
   * everything: HTTP headers, overhead for HTTP chunked encoding, whatever compressed/uncompressed
   * form (i.e. gzip/deflate accept-encoding) the content was sent in, etc. This is NOT the same as
   * the size of the uncompressed FetchReply::Body - if the webserver used gzip encoding, this value
   * might be much smaller, since it only counts the compressed wire size. To illustrate, think of 3
   * sizes: 1) TotalFetchedSize - amount Trawler read over the wire from the server. If they used
   * gzip/deflate, this might be 4-5x smaller than the body. 2) UnTruncatedSize/CutoffSize - how big
   * is the full document, after uncompressing any gzip/deflate encoding? If truncated, this is
   * reflected in CutoffSize. 3) FetchReply::Body size - most crawls enable Trawler compression to
   * save storage space (gzip + a google html dictionary). The body size that the end Trawler client
   * sees is post-compression.
   * @return value or {@code null} for none
   */
  public java.lang.Long getTotalFetchedSize() {
    return totalFetchedSize;
  }

  /**
   * How many raw bytes we read from the connection to the server when we fetched the page. Includes
   * everything: HTTP headers, overhead for HTTP chunked encoding, whatever compressed/uncompressed
   * form (i.e. gzip/deflate accept-encoding) the content was sent in, etc. This is NOT the same as
   * the size of the uncompressed FetchReply::Body - if the webserver used gzip encoding, this value
   * might be much smaller, since it only counts the compressed wire size. To illustrate, think of 3
   * sizes: 1) TotalFetchedSize - amount Trawler read over the wire from the server. If they used
   * gzip/deflate, this might be 4-5x smaller than the body. 2) UnTruncatedSize/CutoffSize - how big
   * is the full document, after uncompressing any gzip/deflate encoding? If truncated, this is
   * reflected in CutoffSize. 3) FetchReply::Body size - most crawls enable Trawler compression to
   * save storage space (gzip + a google html dictionary). The body size that the end Trawler client
   * sees is post-compression.
   * @param totalFetchedSize totalFetchedSize or {@code null} for none
   */
  public TrawlerFetchReplyData setTotalFetchedSize(java.lang.Long totalFetchedSize) {
    this.totalFetchedSize = totalFetchedSize;
    return this;
  }

  /**
   * If the url got rewriten by transparent rewrites, here it is the series of rewrites it got
   * through. The fetched one is the last
   * @return value or {@code null} for none
   */
  public java.util.List<java.lang.String> getTransparentRewrites() {
    return transparentRewrites;
  }

  /**
   * If the url got rewriten by transparent rewrites, here it is the series of rewrites it got
   * through. The fetched one is the last
   * @param transparentRewrites transparentRewrites or {@code null} for none
   */
  public TrawlerFetchReplyData setTransparentRewrites(java.util.List<java.lang.String> transparentRewrites) {
    this.transparentRewrites = transparentRewrites;
    return this;
  }

  /**
   * For logging only; not present in the actual fetcher response
   * @return value or {@code null} for none
   */
  public TrawlerTrawlerPrivateFetchReplyData getTrawlerPrivate() {
    return trawlerPrivate;
  }

  /**
   * For logging only; not present in the actual fetcher response
   * @param trawlerPrivate trawlerPrivate or {@code null} for none
   */
  public TrawlerFetchReplyData setTrawlerPrivate(TrawlerTrawlerPrivateFetchReplyData trawlerPrivate) {
    this.trawlerPrivate = trawlerPrivate;
    return this;
  }

  /**
   * The original url in the request we are answering. Even though "optional," url must be filled in
   * on all well-formed replies. Trawler guarantees that it is filled in, and basically every client
   * expects it (CHECKs in some cases). -> Not filling this field in is a bug, if you share this
   * data with other crawls/pipelines. You should expect everybody else to require a url.
   * @return value or {@code null} for none
   */
  public java.lang.String getUrl() {
    return url;
  }

  /**
   * The original url in the request we are answering. Even though "optional," url must be filled in
   * on all well-formed replies. Trawler guarantees that it is filled in, and basically every client
   * expects it (CHECKs in some cases). -> Not filling this field in is a bug, if you share this
   * data with other crawls/pipelines. You should expect everybody else to require a url.
   * @param url url or {@code null} for none
   */
  public TrawlerFetchReplyData setUrl(java.lang.String url) {
    this.url = url;
    return this;
  }

  /**
   * Encoding info for the original url itself. Bitfield encoding; see UrlEncoding::{Set,Get}Value
   * in webutil/urlencoding.
   * @return value or {@code null} for none
   */
  public java.lang.Integer getUrlEncoding() {
    return urlEncoding;
  }

  /**
   * Encoding info for the original url itself. Bitfield encoding; see UrlEncoding::{Set,Get}Value
   * in webutil/urlencoding.
   * @param urlEncoding urlEncoding or {@code null} for none
   */
  public TrawlerFetchReplyData setUrlEncoding(java.lang.Integer urlEncoding) {
    this.urlEncoding = urlEncoding;
    return this;
  }

  /**
   * Use the special compression dictionary for uncompressing this. (trawler::kHtmlCompressionDict.
   * Use trawler::FetchReplyUncompressor to uncompress; crawler/trawler/public/fetchreply-util.h)
   * @return value or {@code null} for none
   */
  public java.lang.Boolean getUseHtmlCompressDictionary() {
    return useHtmlCompressDictionary;
  }

  /**
   * Use the special compression dictionary for uncompressing this. (trawler::kHtmlCompressionDict.
   * Use trawler::FetchReplyUncompressor to uncompress; crawler/trawler/public/fetchreply-util.h)
   * @param useHtmlCompressDictionary useHtmlCompressDictionary or {@code null} for none
   */
  public TrawlerFetchReplyData setUseHtmlCompressDictionary(java.lang.Boolean useHtmlCompressDictionary) {
    this.useHtmlCompressDictionary = useHtmlCompressDictionary;
    return this;
  }

  /**
   * @return value or {@code null} for none
   */
  public TrawlerFetchReplyDataCrawlDates getCrawldates() {
    return crawldates;
  }

  /**
   * @param crawldates crawldates or {@code null} for none
   */
  public TrawlerFetchReplyData setCrawldates(TrawlerFetchReplyDataCrawlDates crawldates) {
    this.crawldates = crawldates;
    return this;
  }

  /**
   * Transfer operation detailed report.
   * @return value or {@code null} for none
   */
  public TrawlerFetchReplyDataDeliveryReport getDeliveryReport() {
    return deliveryReport;
  }

  /**
   * Transfer operation detailed report.
   * @param deliveryReport deliveryReport or {@code null} for none
   */
  public TrawlerFetchReplyData setDeliveryReport(TrawlerFetchReplyDataDeliveryReport deliveryReport) {
    this.deliveryReport = deliveryReport;
    return this;
  }

  /**
   * @return value or {@code null} for none
   */
  public TrawlerFetchReplyDataFetchStats getFetchstats() {
    return fetchstats;
  }

  /**
   * @param fetchstats fetchstats or {@code null} for none
   */
  public TrawlerFetchReplyData setFetchstats(TrawlerFetchReplyDataFetchStats fetchstats) {
    this.fetchstats = fetchstats;
    return this;
  }

  /**
   * If the input url in FetchRequest is Amazon S3 protocol or Apple Itunes protocol, we will
   * translate it into https url and log it as https url. In the meantime we will store the original
   * s3/itunes url in this field. Before sending back to client, the Url will be translated back to
   * s3 and this field will be cleard.
   * @return value or {@code null} for none
   */
  public java.lang.String getOriginalProtocolUrl() {
    return originalProtocolUrl;
  }

  /**
   * If the input url in FetchRequest is Amazon S3 protocol or Apple Itunes protocol, we will
   * translate it into https url and log it as https url. In the meantime we will store the original
   * s3/itunes url in this field. Before sending back to client, the Url will be translated back to
   * s3 and this field will be cleard.
   * @param originalProtocolUrl originalProtocolUrl or {@code null} for none
   */
  public TrawlerFetchReplyData setOriginalProtocolUrl(java.lang.String originalProtocolUrl) {
    this.originalProtocolUrl = originalProtocolUrl;
    return this;
  }

  /**
   * @return value or {@code null} for none
   */
  public TrawlerFetchReplyDataPartialResponse getPartialresponse() {
    return partialresponse;
  }

  /**
   * @param partialresponse partialresponse or {@code null} for none
   */
  public TrawlerFetchReplyData setPartialresponse(TrawlerFetchReplyDataPartialResponse partialresponse) {
    this.partialresponse = partialresponse;
    return this;
  }

  /**
   * @return value or {@code null} for none
   */
  public TrawlerFetchReplyDataProtocolResponse getProtocolresponse() {
    return protocolresponse;
  }

  /**
   * @param protocolresponse protocolresponse or {@code null} for none
   */
  public TrawlerFetchReplyData setProtocolresponse(TrawlerFetchReplyDataProtocolResponse protocolresponse) {
    this.protocolresponse = protocolresponse;
    return this;
  }

  /**
   * @return value or {@code null} for none
   */
  public java.util.List<TrawlerFetchReplyDataRedirects> getRedirects() {
    return redirects;
  }

  /**
   * @param redirects redirects or {@code null} for none
   */
  public TrawlerFetchReplyData setRedirects(java.util.List<TrawlerFetchReplyDataRedirects> redirects) {
    this.redirects = redirects;
    return this;
  }

  /**
   * Traffic type of this fetch.
   * @return value or {@code null} for none
   */
  public java.lang.String getTrafficType() {
    return trafficType;
  }

  /**
   * Traffic type of this fetch.
   * @param trafficType trafficType or {@code null} for none
   */
  public TrawlerFetchReplyData setTrafficType(java.lang.String trafficType) {
    this.trafficType = trafficType;
    return this;
  }

  @Override
  public TrawlerFetchReplyData set(String fieldName, Object value) {
    return (TrawlerFetchReplyData) super.set(fieldName, value);
  }

  @Override
  public TrawlerFetchReplyData clone() {
    return (TrawlerFetchReplyData) super.clone();
  }

}
