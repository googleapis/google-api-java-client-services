/*
 * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
 * in compliance with the License. You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License
 * is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
 * or implied. See the License for the specific language governing permissions and limitations under
 * the License.
 */
/*
 * This code was generated by https://github.com/googleapis/google-api-java-client-services/
 * Modify at your own risk.
 */

package com.google.api.services.dialogflow.v2.model;

/**
 * The parameters of inference.
 *
 * <p> This is the Java data model class that specifies how to parse/serialize into the JSON that is
 * transmitted over HTTP when working with the Dialogflow API. For a detailed explanation see:
 * <a href="https://developers.google.com/api-client-library/java/google-http-java-client/json">https://developers.google.com/api-client-library/java/google-http-java-client/json</a>
 * </p>
 *
 * @author Google, Inc.
 */
@SuppressWarnings("javadoc")
public final class GoogleCloudDialogflowV2InferenceParameter extends com.google.api.client.json.GenericJson {

  /**
   * Optional. Maximum number of the output tokens for the generator.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.Integer maxOutputTokens;

  /**
   * Optional. Controls the randomness of LLM predictions. Low temperature = less random. High
   * temperature = more random. If unset (or 0), uses a default value of 0.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.Double temperature;

  /**
   * Optional. Top-k changes how the model selects tokens for output. A top-k of 1 means the
   * selected token is the most probable among all tokens in the model's vocabulary (also called
   * greedy decoding), while a top-k of 3 means that the next token is selected from among the 3
   * most probable tokens (using temperature). For each token selection step, the top K tokens with
   * the highest probabilities are sampled. Then tokens are further filtered based on topP with the
   * final token selected using temperature sampling. Specify a lower value for less random
   * responses and a higher value for more random responses. Acceptable value is [1, 40], default to
   * 40.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.Integer topK;

  /**
   * Optional. Top-p changes how the model selects tokens for output. Tokens are selected from most
   * K (see topK parameter) probable to least until the sum of their probabilities equals the top-p
   * value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the top-p
   * value is 0.5, then the model will select either A or B as the next token (using temperature)
   * and doesn't consider C. The default top-p value is 0.95. Specify a lower value for less random
   * responses and a higher value for more random responses. Acceptable value is [0.0, 1.0], default
   * to 0.95.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.Double topP;

  /**
   * Optional. Maximum number of the output tokens for the generator.
   * @return value or {@code null} for none
   */
  public java.lang.Integer getMaxOutputTokens() {
    return maxOutputTokens;
  }

  /**
   * Optional. Maximum number of the output tokens for the generator.
   * @param maxOutputTokens maxOutputTokens or {@code null} for none
   */
  public GoogleCloudDialogflowV2InferenceParameter setMaxOutputTokens(java.lang.Integer maxOutputTokens) {
    this.maxOutputTokens = maxOutputTokens;
    return this;
  }

  /**
   * Optional. Controls the randomness of LLM predictions. Low temperature = less random. High
   * temperature = more random. If unset (or 0), uses a default value of 0.
   * @return value or {@code null} for none
   */
  public java.lang.Double getTemperature() {
    return temperature;
  }

  /**
   * Optional. Controls the randomness of LLM predictions. Low temperature = less random. High
   * temperature = more random. If unset (or 0), uses a default value of 0.
   * @param temperature temperature or {@code null} for none
   */
  public GoogleCloudDialogflowV2InferenceParameter setTemperature(java.lang.Double temperature) {
    this.temperature = temperature;
    return this;
  }

  /**
   * Optional. Top-k changes how the model selects tokens for output. A top-k of 1 means the
   * selected token is the most probable among all tokens in the model's vocabulary (also called
   * greedy decoding), while a top-k of 3 means that the next token is selected from among the 3
   * most probable tokens (using temperature). For each token selection step, the top K tokens with
   * the highest probabilities are sampled. Then tokens are further filtered based on topP with the
   * final token selected using temperature sampling. Specify a lower value for less random
   * responses and a higher value for more random responses. Acceptable value is [1, 40], default to
   * 40.
   * @return value or {@code null} for none
   */
  public java.lang.Integer getTopK() {
    return topK;
  }

  /**
   * Optional. Top-k changes how the model selects tokens for output. A top-k of 1 means the
   * selected token is the most probable among all tokens in the model's vocabulary (also called
   * greedy decoding), while a top-k of 3 means that the next token is selected from among the 3
   * most probable tokens (using temperature). For each token selection step, the top K tokens with
   * the highest probabilities are sampled. Then tokens are further filtered based on topP with the
   * final token selected using temperature sampling. Specify a lower value for less random
   * responses and a higher value for more random responses. Acceptable value is [1, 40], default to
   * 40.
   * @param topK topK or {@code null} for none
   */
  public GoogleCloudDialogflowV2InferenceParameter setTopK(java.lang.Integer topK) {
    this.topK = topK;
    return this;
  }

  /**
   * Optional. Top-p changes how the model selects tokens for output. Tokens are selected from most
   * K (see topK parameter) probable to least until the sum of their probabilities equals the top-p
   * value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the top-p
   * value is 0.5, then the model will select either A or B as the next token (using temperature)
   * and doesn't consider C. The default top-p value is 0.95. Specify a lower value for less random
   * responses and a higher value for more random responses. Acceptable value is [0.0, 1.0], default
   * to 0.95.
   * @return value or {@code null} for none
   */
  public java.lang.Double getTopP() {
    return topP;
  }

  /**
   * Optional. Top-p changes how the model selects tokens for output. Tokens are selected from most
   * K (see topK parameter) probable to least until the sum of their probabilities equals the top-p
   * value. For example, if tokens A, B, and C have a probability of 0.3, 0.2, and 0.1 and the top-p
   * value is 0.5, then the model will select either A or B as the next token (using temperature)
   * and doesn't consider C. The default top-p value is 0.95. Specify a lower value for less random
   * responses and a higher value for more random responses. Acceptable value is [0.0, 1.0], default
   * to 0.95.
   * @param topP topP or {@code null} for none
   */
  public GoogleCloudDialogflowV2InferenceParameter setTopP(java.lang.Double topP) {
    this.topP = topP;
    return this;
  }

  @Override
  public GoogleCloudDialogflowV2InferenceParameter set(String fieldName, Object value) {
    return (GoogleCloudDialogflowV2InferenceParameter) super.set(fieldName, value);
  }

  @Override
  public GoogleCloudDialogflowV2InferenceParameter clone() {
    return (GoogleCloudDialogflowV2InferenceParameter) super.clone();
  }

}
