/*
 * Licensed under the Apache License, Version 2.0 (the "License"); you may not use this file except
 * in compliance with the License. You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software distributed under the License
 * is distributed on an "AS IS" BASIS, WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express
 * or implied. See the License for the specific language governing permissions and limitations under
 * the License.
 */
/*
 * This code was generated by https://github.com/googleapis/google-api-java-client-services/
 * Modify at your own risk.
 */

package com.google.api.services.firebaseml.v2beta.model;

/**
 * Configuration for content generation. This message contains all the parameters that control how
 * the model generates content. It allows you to influence the randomness, length, and structure of
 * the output.
 *
 * <p> This is the Java data model class that specifies how to parse/serialize into the JSON that is
 * transmitted over HTTP when working with the Firebase ML API. For a detailed explanation see:
 * <a href="https://developers.google.com/api-client-library/java/google-http-java-client/json">https://developers.google.com/api-client-library/java/google-http-java-client/json</a>
 * </p>
 *
 * @author Google, Inc.
 */
@SuppressWarnings("javadoc")
public final class GoogleCloudAiplatformV1beta1GenerationConfig extends com.google.api.client.json.GenericJson {

  /**
   * Optional. If enabled, audio timestamps will be included in the request to the model. This can
   * be useful for synchronizing audio with other modalities in the response.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.Boolean audioTimestamp;

  /**
   * Optional. The number of candidate responses to generate. A higher `candidate_count` can provide
   * more options to choose from, but it also consumes more resources. This can be useful for
   * generating a variety of responses and selecting the best one.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.Integer candidateCount;

  /**
   * Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For
   * example, if the model detects that the user is frustrated, it may provide a more empathetic
   * response.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.Boolean enableAffectiveDialog;

  /**
   * Optional. Penalizes tokens based on their frequency in the generated text. A positive value
   * helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.Float frequencyPenalty;

  /**
   * Optional. Config for image generation features.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private GoogleCloudAiplatformV1beta1ImageConfig imageConfig;

  /**
   * Optional. The number of top log probabilities to return for each token. This can be used to see
   * which other tokens were considered likely candidates for a given position. A higher value will
   * return more options, but it will also increase the size of the response.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.Integer logprobs;

  /**
   * Optional. The maximum number of tokens to generate in the response. A token is approximately
   * four characters. The default value varies by model. This parameter can be used to control the
   * length of the generated text and prevent overly long responses.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.Integer maxOutputTokens;

  /**
   * Optional. The token resolution at which input media content is sampled. This is used to control
   * the trade-off between the quality of the response and the number of tokens used to represent
   * the media. A higher resolution allows the model to perceive more detail, which can lead to a
   * more nuanced response, but it will also use more tokens. This does not affect the image
   * dimensions sent to the model.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.String mediaResolution;

  /**
   * Optional. Config for model selection.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private GoogleCloudAiplatformV1beta1GenerationConfigModelConfig modelConfig;

  /**
   * Optional. Penalizes tokens that have already appeared in the generated text. A positive value
   * encourages the model to generate more diverse and less repetitive text. Valid values can range
   * from [-2.0, 2.0].
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.Float presencePenalty;

  /**
   * Optional. When this field is set, response_schema must be omitted and response_mime_type must
   * be set to `application/json`.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.Object responseJsonSchema;

  /**
   * Optional. If set to true, the log probabilities of the output tokens are returned. Log
   * probabilities are the logarithm of the probability of a token appearing in the output. A higher
   * log probability means the token is more likely to be generated. This can be useful for
   * analyzing the model's confidence in its own output and for debugging.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.Boolean responseLogprobs;

  /**
   * Optional. The IANA standard MIME type of the response. The model will generate output that
   * conforms to this MIME type. Supported values include 'text/plain' (default) and
   * 'application/json'. The model needs to be prompted to output the appropriate response type,
   * otherwise the behavior is undefined. This is a preview feature.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.String responseMimeType;

  /**
   * Optional. The modalities of the response. The model will generate a response that includes all
   * the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will
   * include both text and an image.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.util.List<java.lang.String> responseModalities;

  /**
   * Optional. Lets you to specify a schema for the model's response, ensuring that the output
   * conforms to a particular structure. This is useful for generating structured data such as JSON.
   * The schema is a subset of the [OpenAPI 3.0 schema
   * object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must
   * also set the `response_mime_type` to `application/json`.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private GoogleCloudAiplatformV1beta1Schema responseSchema;

  /**
   * Optional. Routing configuration.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private GoogleCloudAiplatformV1beta1GenerationConfigRoutingConfig routingConfig;

  /**
   * Optional. A seed for the random number generator. By setting a seed, you can make the model's
   * output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.),
   * the model will produce the same response every time. However, it's not a guaranteed absolute
   * deterministic behavior. This is different from parameters like `temperature`, which control the
   * *level* of randomness. `seed` ensures that the "random" choices the model makes are the same on
   * every run, making it essential for testing and ensuring reproducible results.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.Integer seed;

  /**
   * Optional. The speech generation config.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private GoogleCloudAiplatformV1beta1SpeechConfig speechConfig;

  /**
   * Optional. A list of character sequences that will stop the model from generating further
   * tokens. If a stop sequence is generated, the output will end at that point. This is useful for
   * controlling the length and structure of the output. For example, you can use ["\n", "###"] to
   * stop generation at a new line or a specific marker.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.util.List<java.lang.String> stopSequences;

  /**
   * Optional. Controls the randomness of the output. A higher temperature results in more creative
   * and diverse responses, while a lower temperature makes the output more predictable and focused.
   * The valid range is (0.0, 2.0].
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.Float temperature;

  /**
   * Optional. Configuration for thinking features. An error will be returned if this field is set
   * for models that don't support thinking.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private GoogleCloudAiplatformV1beta1GenerationConfigThinkingConfig thinkingConfig;

  /**
   * Optional. Specifies the top-k sampling threshold. The model considers only the top k most
   * probable tokens for the next token. This can be useful for generating more coherent and less
   * random text. For example, a `top_k` of 40 means the model will choose the next word from the 40
   * most likely words.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.Float topK;

  /**
   * Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set
   * of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse
   * and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens
   * until the cumulative probability of the tokens to select from reaches 0.9. It's recommended to
   * adjust either temperature or `top_p`, but not both.
   * The value may be {@code null}.
   */
  @com.google.api.client.util.Key
  private java.lang.Float topP;

  /**
   * Optional. If enabled, audio timestamps will be included in the request to the model. This can
   * be useful for synchronizing audio with other modalities in the response.
   * @return value or {@code null} for none
   */
  public java.lang.Boolean getAudioTimestamp() {
    return audioTimestamp;
  }

  /**
   * Optional. If enabled, audio timestamps will be included in the request to the model. This can
   * be useful for synchronizing audio with other modalities in the response.
   * @param audioTimestamp audioTimestamp or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GenerationConfig setAudioTimestamp(java.lang.Boolean audioTimestamp) {
    this.audioTimestamp = audioTimestamp;
    return this;
  }

  /**
   * Optional. The number of candidate responses to generate. A higher `candidate_count` can provide
   * more options to choose from, but it also consumes more resources. This can be useful for
   * generating a variety of responses and selecting the best one.
   * @return value or {@code null} for none
   */
  public java.lang.Integer getCandidateCount() {
    return candidateCount;
  }

  /**
   * Optional. The number of candidate responses to generate. A higher `candidate_count` can provide
   * more options to choose from, but it also consumes more resources. This can be useful for
   * generating a variety of responses and selecting the best one.
   * @param candidateCount candidateCount or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GenerationConfig setCandidateCount(java.lang.Integer candidateCount) {
    this.candidateCount = candidateCount;
    return this;
  }

  /**
   * Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For
   * example, if the model detects that the user is frustrated, it may provide a more empathetic
   * response.
   * @return value or {@code null} for none
   */
  public java.lang.Boolean getEnableAffectiveDialog() {
    return enableAffectiveDialog;
  }

  /**
   * Optional. If enabled, the model will detect emotions and adapt its responses accordingly. For
   * example, if the model detects that the user is frustrated, it may provide a more empathetic
   * response.
   * @param enableAffectiveDialog enableAffectiveDialog or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GenerationConfig setEnableAffectiveDialog(java.lang.Boolean enableAffectiveDialog) {
    this.enableAffectiveDialog = enableAffectiveDialog;
    return this;
  }

  /**
   * Optional. Penalizes tokens based on their frequency in the generated text. A positive value
   * helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
   * @return value or {@code null} for none
   */
  public java.lang.Float getFrequencyPenalty() {
    return frequencyPenalty;
  }

  /**
   * Optional. Penalizes tokens based on their frequency in the generated text. A positive value
   * helps to reduce the repetition of words and phrases. Valid values can range from [-2.0, 2.0].
   * @param frequencyPenalty frequencyPenalty or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GenerationConfig setFrequencyPenalty(java.lang.Float frequencyPenalty) {
    this.frequencyPenalty = frequencyPenalty;
    return this;
  }

  /**
   * Optional. Config for image generation features.
   * @return value or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1ImageConfig getImageConfig() {
    return imageConfig;
  }

  /**
   * Optional. Config for image generation features.
   * @param imageConfig imageConfig or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GenerationConfig setImageConfig(GoogleCloudAiplatformV1beta1ImageConfig imageConfig) {
    this.imageConfig = imageConfig;
    return this;
  }

  /**
   * Optional. The number of top log probabilities to return for each token. This can be used to see
   * which other tokens were considered likely candidates for a given position. A higher value will
   * return more options, but it will also increase the size of the response.
   * @return value or {@code null} for none
   */
  public java.lang.Integer getLogprobs() {
    return logprobs;
  }

  /**
   * Optional. The number of top log probabilities to return for each token. This can be used to see
   * which other tokens were considered likely candidates for a given position. A higher value will
   * return more options, but it will also increase the size of the response.
   * @param logprobs logprobs or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GenerationConfig setLogprobs(java.lang.Integer logprobs) {
    this.logprobs = logprobs;
    return this;
  }

  /**
   * Optional. The maximum number of tokens to generate in the response. A token is approximately
   * four characters. The default value varies by model. This parameter can be used to control the
   * length of the generated text and prevent overly long responses.
   * @return value or {@code null} for none
   */
  public java.lang.Integer getMaxOutputTokens() {
    return maxOutputTokens;
  }

  /**
   * Optional. The maximum number of tokens to generate in the response. A token is approximately
   * four characters. The default value varies by model. This parameter can be used to control the
   * length of the generated text and prevent overly long responses.
   * @param maxOutputTokens maxOutputTokens or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GenerationConfig setMaxOutputTokens(java.lang.Integer maxOutputTokens) {
    this.maxOutputTokens = maxOutputTokens;
    return this;
  }

  /**
   * Optional. The token resolution at which input media content is sampled. This is used to control
   * the trade-off between the quality of the response and the number of tokens used to represent
   * the media. A higher resolution allows the model to perceive more detail, which can lead to a
   * more nuanced response, but it will also use more tokens. This does not affect the image
   * dimensions sent to the model.
   * @return value or {@code null} for none
   */
  public java.lang.String getMediaResolution() {
    return mediaResolution;
  }

  /**
   * Optional. The token resolution at which input media content is sampled. This is used to control
   * the trade-off between the quality of the response and the number of tokens used to represent
   * the media. A higher resolution allows the model to perceive more detail, which can lead to a
   * more nuanced response, but it will also use more tokens. This does not affect the image
   * dimensions sent to the model.
   * @param mediaResolution mediaResolution or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GenerationConfig setMediaResolution(java.lang.String mediaResolution) {
    this.mediaResolution = mediaResolution;
    return this;
  }

  /**
   * Optional. Config for model selection.
   * @return value or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GenerationConfigModelConfig getModelConfig() {
    return modelConfig;
  }

  /**
   * Optional. Config for model selection.
   * @param modelConfig modelConfig or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GenerationConfig setModelConfig(GoogleCloudAiplatformV1beta1GenerationConfigModelConfig modelConfig) {
    this.modelConfig = modelConfig;
    return this;
  }

  /**
   * Optional. Penalizes tokens that have already appeared in the generated text. A positive value
   * encourages the model to generate more diverse and less repetitive text. Valid values can range
   * from [-2.0, 2.0].
   * @return value or {@code null} for none
   */
  public java.lang.Float getPresencePenalty() {
    return presencePenalty;
  }

  /**
   * Optional. Penalizes tokens that have already appeared in the generated text. A positive value
   * encourages the model to generate more diverse and less repetitive text. Valid values can range
   * from [-2.0, 2.0].
   * @param presencePenalty presencePenalty or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GenerationConfig setPresencePenalty(java.lang.Float presencePenalty) {
    this.presencePenalty = presencePenalty;
    return this;
  }

  /**
   * Optional. When this field is set, response_schema must be omitted and response_mime_type must
   * be set to `application/json`.
   * @return value or {@code null} for none
   */
  public java.lang.Object getResponseJsonSchema() {
    return responseJsonSchema;
  }

  /**
   * Optional. When this field is set, response_schema must be omitted and response_mime_type must
   * be set to `application/json`.
   * @param responseJsonSchema responseJsonSchema or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GenerationConfig setResponseJsonSchema(java.lang.Object responseJsonSchema) {
    this.responseJsonSchema = responseJsonSchema;
    return this;
  }

  /**
   * Optional. If set to true, the log probabilities of the output tokens are returned. Log
   * probabilities are the logarithm of the probability of a token appearing in the output. A higher
   * log probability means the token is more likely to be generated. This can be useful for
   * analyzing the model's confidence in its own output and for debugging.
   * @return value or {@code null} for none
   */
  public java.lang.Boolean getResponseLogprobs() {
    return responseLogprobs;
  }

  /**
   * Optional. If set to true, the log probabilities of the output tokens are returned. Log
   * probabilities are the logarithm of the probability of a token appearing in the output. A higher
   * log probability means the token is more likely to be generated. This can be useful for
   * analyzing the model's confidence in its own output and for debugging.
   * @param responseLogprobs responseLogprobs or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GenerationConfig setResponseLogprobs(java.lang.Boolean responseLogprobs) {
    this.responseLogprobs = responseLogprobs;
    return this;
  }

  /**
   * Optional. The IANA standard MIME type of the response. The model will generate output that
   * conforms to this MIME type. Supported values include 'text/plain' (default) and
   * 'application/json'. The model needs to be prompted to output the appropriate response type,
   * otherwise the behavior is undefined. This is a preview feature.
   * @return value or {@code null} for none
   */
  public java.lang.String getResponseMimeType() {
    return responseMimeType;
  }

  /**
   * Optional. The IANA standard MIME type of the response. The model will generate output that
   * conforms to this MIME type. Supported values include 'text/plain' (default) and
   * 'application/json'. The model needs to be prompted to output the appropriate response type,
   * otherwise the behavior is undefined. This is a preview feature.
   * @param responseMimeType responseMimeType or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GenerationConfig setResponseMimeType(java.lang.String responseMimeType) {
    this.responseMimeType = responseMimeType;
    return this;
  }

  /**
   * Optional. The modalities of the response. The model will generate a response that includes all
   * the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will
   * include both text and an image.
   * @return value or {@code null} for none
   */
  public java.util.List<java.lang.String> getResponseModalities() {
    return responseModalities;
  }

  /**
   * Optional. The modalities of the response. The model will generate a response that includes all
   * the specified modalities. For example, if this is set to `[TEXT, IMAGE]`, the response will
   * include both text and an image.
   * @param responseModalities responseModalities or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GenerationConfig setResponseModalities(java.util.List<java.lang.String> responseModalities) {
    this.responseModalities = responseModalities;
    return this;
  }

  /**
   * Optional. Lets you to specify a schema for the model's response, ensuring that the output
   * conforms to a particular structure. This is useful for generating structured data such as JSON.
   * The schema is a subset of the [OpenAPI 3.0 schema
   * object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must
   * also set the `response_mime_type` to `application/json`.
   * @return value or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1Schema getResponseSchema() {
    return responseSchema;
  }

  /**
   * Optional. Lets you to specify a schema for the model's response, ensuring that the output
   * conforms to a particular structure. This is useful for generating structured data such as JSON.
   * The schema is a subset of the [OpenAPI 3.0 schema
   * object](https://spec.openapis.org/oas/v3.0.3#schema) object. When this field is set, you must
   * also set the `response_mime_type` to `application/json`.
   * @param responseSchema responseSchema or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GenerationConfig setResponseSchema(GoogleCloudAiplatformV1beta1Schema responseSchema) {
    this.responseSchema = responseSchema;
    return this;
  }

  /**
   * Optional. Routing configuration.
   * @return value or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GenerationConfigRoutingConfig getRoutingConfig() {
    return routingConfig;
  }

  /**
   * Optional. Routing configuration.
   * @param routingConfig routingConfig or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GenerationConfig setRoutingConfig(GoogleCloudAiplatformV1beta1GenerationConfigRoutingConfig routingConfig) {
    this.routingConfig = routingConfig;
    return this;
  }

  /**
   * Optional. A seed for the random number generator. By setting a seed, you can make the model's
   * output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.),
   * the model will produce the same response every time. However, it's not a guaranteed absolute
   * deterministic behavior. This is different from parameters like `temperature`, which control the
   * *level* of randomness. `seed` ensures that the "random" choices the model makes are the same on
   * every run, making it essential for testing and ensuring reproducible results.
   * @return value or {@code null} for none
   */
  public java.lang.Integer getSeed() {
    return seed;
  }

  /**
   * Optional. A seed for the random number generator. By setting a seed, you can make the model's
   * output mostly deterministic. For a given prompt and parameters (like temperature, top_p, etc.),
   * the model will produce the same response every time. However, it's not a guaranteed absolute
   * deterministic behavior. This is different from parameters like `temperature`, which control the
   * *level* of randomness. `seed` ensures that the "random" choices the model makes are the same on
   * every run, making it essential for testing and ensuring reproducible results.
   * @param seed seed or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GenerationConfig setSeed(java.lang.Integer seed) {
    this.seed = seed;
    return this;
  }

  /**
   * Optional. The speech generation config.
   * @return value or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1SpeechConfig getSpeechConfig() {
    return speechConfig;
  }

  /**
   * Optional. The speech generation config.
   * @param speechConfig speechConfig or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GenerationConfig setSpeechConfig(GoogleCloudAiplatformV1beta1SpeechConfig speechConfig) {
    this.speechConfig = speechConfig;
    return this;
  }

  /**
   * Optional. A list of character sequences that will stop the model from generating further
   * tokens. If a stop sequence is generated, the output will end at that point. This is useful for
   * controlling the length and structure of the output. For example, you can use ["\n", "###"] to
   * stop generation at a new line or a specific marker.
   * @return value or {@code null} for none
   */
  public java.util.List<java.lang.String> getStopSequences() {
    return stopSequences;
  }

  /**
   * Optional. A list of character sequences that will stop the model from generating further
   * tokens. If a stop sequence is generated, the output will end at that point. This is useful for
   * controlling the length and structure of the output. For example, you can use ["\n", "###"] to
   * stop generation at a new line or a specific marker.
   * @param stopSequences stopSequences or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GenerationConfig setStopSequences(java.util.List<java.lang.String> stopSequences) {
    this.stopSequences = stopSequences;
    return this;
  }

  /**
   * Optional. Controls the randomness of the output. A higher temperature results in more creative
   * and diverse responses, while a lower temperature makes the output more predictable and focused.
   * The valid range is (0.0, 2.0].
   * @return value or {@code null} for none
   */
  public java.lang.Float getTemperature() {
    return temperature;
  }

  /**
   * Optional. Controls the randomness of the output. A higher temperature results in more creative
   * and diverse responses, while a lower temperature makes the output more predictable and focused.
   * The valid range is (0.0, 2.0].
   * @param temperature temperature or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GenerationConfig setTemperature(java.lang.Float temperature) {
    this.temperature = temperature;
    return this;
  }

  /**
   * Optional. Configuration for thinking features. An error will be returned if this field is set
   * for models that don't support thinking.
   * @return value or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GenerationConfigThinkingConfig getThinkingConfig() {
    return thinkingConfig;
  }

  /**
   * Optional. Configuration for thinking features. An error will be returned if this field is set
   * for models that don't support thinking.
   * @param thinkingConfig thinkingConfig or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GenerationConfig setThinkingConfig(GoogleCloudAiplatformV1beta1GenerationConfigThinkingConfig thinkingConfig) {
    this.thinkingConfig = thinkingConfig;
    return this;
  }

  /**
   * Optional. Specifies the top-k sampling threshold. The model considers only the top k most
   * probable tokens for the next token. This can be useful for generating more coherent and less
   * random text. For example, a `top_k` of 40 means the model will choose the next word from the 40
   * most likely words.
   * @return value or {@code null} for none
   */
  public java.lang.Float getTopK() {
    return topK;
  }

  /**
   * Optional. Specifies the top-k sampling threshold. The model considers only the top k most
   * probable tokens for the next token. This can be useful for generating more coherent and less
   * random text. For example, a `top_k` of 40 means the model will choose the next word from the 40
   * most likely words.
   * @param topK topK or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GenerationConfig setTopK(java.lang.Float topK) {
    this.topK = topK;
    return this;
  }

  /**
   * Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set
   * of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse
   * and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens
   * until the cumulative probability of the tokens to select from reaches 0.9. It's recommended to
   * adjust either temperature or `top_p`, but not both.
   * @return value or {@code null} for none
   */
  public java.lang.Float getTopP() {
    return topP;
  }

  /**
   * Optional. Specifies the nucleus sampling threshold. The model considers only the smallest set
   * of tokens whose cumulative probability is at least `top_p`. This helps generate more diverse
   * and less repetitive responses. For example, a `top_p` of 0.9 means the model considers tokens
   * until the cumulative probability of the tokens to select from reaches 0.9. It's recommended to
   * adjust either temperature or `top_p`, but not both.
   * @param topP topP or {@code null} for none
   */
  public GoogleCloudAiplatformV1beta1GenerationConfig setTopP(java.lang.Float topP) {
    this.topP = topP;
    return this;
  }

  @Override
  public GoogleCloudAiplatformV1beta1GenerationConfig set(String fieldName, Object value) {
    return (GoogleCloudAiplatformV1beta1GenerationConfig) super.set(fieldName, value);
  }

  @Override
  public GoogleCloudAiplatformV1beta1GenerationConfig clone() {
    return (GoogleCloudAiplatformV1beta1GenerationConfig) super.clone();
  }

}
